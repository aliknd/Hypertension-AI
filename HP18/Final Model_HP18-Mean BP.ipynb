{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45a18412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  time data_type  value\n",
      "0  2024-04-28 00:00:00        hr     67\n",
      "1  2024-04-28 00:01:00        hr     67\n",
      "2  2024-04-28 00:02:00        hr     70\n",
      "3  2024-04-28 00:03:00        hr     66\n",
      "4  2024-04-28 00:04:00        hr     67\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"hp18_hr.csv\"  # Replace with your actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert the time column to datetime format and remove timezone information\n",
    "df['time'] = pd.to_datetime(df['time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Sort the dataframe by time in increasing order\n",
    "df_sorted = df.sort_values(by='time')\n",
    "\n",
    "# Display the first few rows of the sorted dataframe\n",
    "print(df_sorted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02f17bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  time data_type  value\n",
      "0  2024-04-28 02:13:00     steps     33\n",
      "1  2024-04-28 02:14:00     steps     19\n",
      "2  2024-04-28 04:33:00     steps     18\n",
      "3  2024-04-28 04:35:00     steps     17\n",
      "4  2024-04-28 04:36:00     steps     41\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the steps dataset\n",
    "file_path_steps = \"hp18_steps.csv\"  # Replace with your actual file path\n",
    "df_steps = pd.read_csv(file_path_steps)\n",
    "\n",
    "# Convert the time column to datetime format and remove timezone information\n",
    "df_steps['time'] = pd.to_datetime(df_steps['time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Sort the dataframe by time in increasing order\n",
    "df_steps_sorted = df_steps.sort_values(by='time')\n",
    "\n",
    "# Display the first few rows of the sorted dataframe\n",
    "print(df_steps_sorted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bac0d4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 136\n",
      "Number of BP spikes: 126\n",
      "       datetime_local  systolic  diastolic  BP_spike\n",
      "0 2024-04-27 14:54:05       152        101         1\n",
      "1 2024-04-27 20:35:43       149         97         1\n",
      "2 2024-04-28 04:54:57       140         98         1\n",
      "4 2024-04-28 06:57:35       143         95         1\n",
      "3 2024-04-28 06:58:50       142         90         1\n"
     ]
    }
   ],
   "source": [
    "# Load the blood pressure dataset\n",
    "file_path_bp = \"blood_pressure_readings_ID18_cleaned.csv\"  # Replace with your actual file path\n",
    "df_bp = pd.read_csv(file_path_bp)\n",
    "\n",
    "# Select relevant columns\n",
    "df_bp = df_bp[['datetime_local', 'systolic', 'diastolic']]\n",
    "\n",
    "# Convert the datetime column to datetime format\n",
    "df_bp['datetime_local'] = pd.to_datetime(df_bp['datetime_local'])\n",
    "\n",
    "# Sort the dataframe by datetime in increasing order\n",
    "df_bp_sorted = df_bp.sort_values(by='datetime_local')\n",
    "\n",
    "# Add a binary classification column for BP spikes\n",
    "df_bp_sorted['BP_spike'] = ((df_bp_sorted['systolic'] > 130) | (df_bp_sorted['diastolic'] > 80)).astype(int)\n",
    "\n",
    "# Count the number of BP spikes and total records\n",
    "total_records = len(df_bp_sorted)\n",
    "bp_spike_count = df_bp_sorted['BP_spike'].sum()\n",
    "\n",
    "# Print summary\n",
    "print(f\"Total records: {total_records}\")\n",
    "print(f\"Number of BP spikes: {bp_spike_count}\")\n",
    "\n",
    "# Display the first few rows of the processed dataframe\n",
    "print(df_bp_sorted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "554387f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     local_created_at  stressLevel_value\n",
      "0 2024-04-27 15:01:00                  3\n",
      "1 2024-04-28 04:56:00                  2\n",
      "2 2024-04-28 07:00:00                  3\n",
      "3 2024-04-28 11:52:00                  4\n",
      "4 2024-04-28 15:09:00                  3\n"
     ]
    }
   ],
   "source": [
    "# Load the stress data dataset\n",
    "file_path_stress = \"questionnaire_responses_ID18.csv\"  # Replace with your actual file path\n",
    "df_stress = pd.read_csv(file_path_stress)\n",
    "\n",
    "# Select relevant columns\n",
    "df_stress = df_stress[['local_created_at', 'stressLevel_value']]\n",
    "\n",
    "# Convert the time column to datetime format\n",
    "df_stress['local_created_at'] = pd.to_datetime(df_stress['local_created_at'])\n",
    "\n",
    "# Sort the dataframe by time in increasing order\n",
    "df_stress_sorted = df_stress.sort_values(by='local_created_at')\n",
    "\n",
    "# Display the first few rows of the processed dataframe\n",
    "print(df_stress_sorted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bbcb8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Final dataset saved as 'processed_bp_prediction_data_full.csv'.\n",
      "      id  user_id    reading_id             datetime      datetime_local  \\\n",
      "0  792.0     18.0  1.714316e+12  2024-04-28 14:54:57 2024-04-28 04:54:57   \n",
      "1  794.0     18.0  1.714323e+12  2024-04-28 16:57:35 2024-04-28 06:57:35   \n",
      "2  793.0     18.0  1.714324e+12  2024-04-28 16:58:50 2024-04-28 06:58:50   \n",
      "3  796.0     18.0  1.714341e+12  2024-04-28 21:49:18 2024-04-28 11:49:18   \n",
      "4  795.0     18.0  1.714341e+12  2024-04-28 21:50:36 2024-04-28 11:50:36   \n",
      "\n",
      "   pulse device_type           created_at  BP_spike_mean                time  \\\n",
      "0   67.0      BP8000  2024-04-28 15:00:10            1.0 2024-04-28 04:54:00   \n",
      "1   65.0      BP8000  2024-04-28 17:05:11            1.0 2024-04-28 06:57:00   \n",
      "2   69.0      BP8000  2024-04-28 17:05:11            1.0 2024-04-28 06:58:00   \n",
      "3   68.0      BP8000  2024-04-28 21:55:09            1.0 2024-04-28 11:49:00   \n",
      "4   64.0      BP8000  2024-04-28 21:55:09            1.0 2024-04-28 11:50:00   \n",
      "\n",
      "   ... hr_mean_rolling_3  steps_total_rolling_5 hr_std_rolling_3  \\\n",
      "0  ...         68.866667                  218.6         2.977053   \n",
      "1  ...         68.866667                  218.6         2.977053   \n",
      "2  ...         68.866667                  218.6         2.977053   \n",
      "3  ...         70.133333                  218.6         2.155387   \n",
      "4  ...         71.200000                  218.6         2.421997   \n",
      "\n",
      "   cumulative_stress_30min  cumulative_steps_30min  hour_of_day  day_of_week  \\\n",
      "0                      8.0                   358.0          4.0          6.0   \n",
      "1                      8.0                   358.0          6.0          6.0   \n",
      "2                      8.0                   358.0          6.0          6.0   \n",
      "3                      8.0                   547.0         11.0          6.0   \n",
      "4                      8.0                   817.0         11.0          6.0   \n",
      "\n",
      "   is_working_hours  is_weekend  time_since_last_BP_spike  \n",
      "0                 0           1                122.633333  \n",
      "1                 0           1                122.633333  \n",
      "2                 0           1                  1.250000  \n",
      "3                 1           1                290.466667  \n",
      "4                 1           1                  1.300000  \n",
      "\n",
      "[5 rows x 84 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ðŸ“Œ **Step 1: Load Datasets**\n",
    "file_path_hr = \"hp18_hr.csv\"\n",
    "file_path_steps = \"hp18_steps.csv\"\n",
    "file_path_bp = \"blood_pressure_readings_ID18_cleaned.csv\"\n",
    "file_path_stress = \"questionnaire_responses_ID18.csv\"\n",
    "\n",
    "df_hr = pd.read_csv(file_path_hr)\n",
    "df_steps = pd.read_csv(file_path_steps)\n",
    "df_bp = pd.read_csv(file_path_bp)\n",
    "df_stress = pd.read_csv(file_path_stress)\n",
    "\n",
    "# ðŸ“Œ **Step 2: Convert timestamps to uniform `datetime64[ns]`**\n",
    "df_hr['time'] = pd.to_datetime(df_hr['time']).dt.tz_localize(None)\n",
    "df_steps['time'] = pd.to_datetime(df_steps['time']).dt.tz_localize(None)\n",
    "df_bp['datetime_local'] = pd.to_datetime(df_bp['datetime_local']).dt.tz_localize(None)\n",
    "df_stress['local_created_at'] = pd.to_datetime(df_stress['local_created_at']).dt.tz_localize(None)\n",
    "\n",
    "# ðŸ“Œ **Step 3: Remove Data from `2024-10-23` for BP & Stress**\n",
    "df_bp = df_bp[df_bp['datetime_local'].dt.date > pd.to_datetime(\"2024-04-27\").date()]\n",
    "df_stress = df_stress[df_stress['local_created_at'].dt.date > pd.to_datetime(\"2024-04-27\").date()]\n",
    "\n",
    "# ðŸ“Œ **Step 4: Sort datasets by time**\n",
    "df_hr_sorted = df_hr.sort_values(by='time')\n",
    "df_steps_sorted = df_steps.sort_values(by='time')\n",
    "df_bp_sorted = df_bp.sort_values(by='datetime_local')\n",
    "df_stress_sorted = df_stress.sort_values(by='local_created_at')\n",
    "\n",
    "# ðŸ“Œ **Step 5: Add BP spike classification**\n",
    "# Retain the threshold-based definition for reference:\n",
    "df_bp_sorted['BP_spike_threshold'] = ((df_bp_sorted['systolic'] > 130) | (df_bp_sorted['diastolic'] > 80)).astype(int)\n",
    "\n",
    "# New binary definition based on mean values:\n",
    "mean_systolic = df_bp_sorted['systolic'].mean()\n",
    "mean_diastolic = df_bp_sorted['diastolic'].mean()\n",
    "df_bp_sorted['BP_spike_mean'] = ((df_bp_sorted['systolic'] > mean_systolic) | (df_bp_sorted['diastolic'] > mean_diastolic)).astype(int)\n",
    "\n",
    "# Tertile classification for systolic and diastolic:\n",
    "systolic_lower = df_bp_sorted['systolic'].quantile(0.333)\n",
    "systolic_upper = df_bp_sorted['systolic'].quantile(0.667)\n",
    "diastolic_lower = df_bp_sorted['diastolic'].quantile(0.333)\n",
    "diastolic_upper = df_bp_sorted['diastolic'].quantile(0.667)\n",
    "\n",
    "def classify_bp(value, lower, upper):\n",
    "    if value < lower:\n",
    "        return \"low\"\n",
    "    elif value < upper:\n",
    "        return \"mid\"\n",
    "    else:\n",
    "        return \"high\"\n",
    "\n",
    "df_bp_sorted['systolic_tertile'] = df_bp_sorted['systolic'].apply(lambda x: classify_bp(x, systolic_lower, systolic_upper))\n",
    "df_bp_sorted['diastolic_tertile'] = df_bp_sorted['diastolic'].apply(lambda x: classify_bp(x, diastolic_lower, diastolic_upper))\n",
    "\n",
    "# ðŸ“Œ **Step 6: Merge HR & Steps Data Based on Nearest Timestamp**\n",
    "df_biosignals = pd.merge_asof(df_hr_sorted, df_steps_sorted, on='time', direction='backward', suffixes=('_hr', '_steps'))\n",
    "\n",
    "# ðŸ“Œ **Step 7: Compute Rolling Window Statistics for HR & Steps**\n",
    "df_biosignals.set_index('time', inplace=True)\n",
    "time_windows = [5, 10, 30, 60]  # Define time windows (minutes)\n",
    "\n",
    "for window in time_windows:\n",
    "    window_str = f\"{window}min\"\n",
    "    # Rolling windows here include the current row; use .shift(1) later if strictly previous data is needed.\n",
    "    df_biosignals[f'hr_mean_{window_str}'] = df_biosignals['value_hr'].rolling(f\"{window}min\").mean()\n",
    "    df_biosignals[f'hr_min_{window_str}'] = df_biosignals['value_hr'].rolling(f\"{window}min\").min()\n",
    "    df_biosignals[f'hr_max_{window_str}'] = df_biosignals['value_hr'].rolling(f\"{window}min\").max()\n",
    "    df_biosignals[f'hr_std_{window_str}'] = df_biosignals['value_hr'].rolling(f\"{window}min\").std()\n",
    "\n",
    "    df_biosignals[f'steps_total_{window_str}'] = df_biosignals['value_steps'].rolling(f\"{window}min\").sum()\n",
    "    df_biosignals[f'steps_mean_{window_str}'] = df_biosignals['value_steps'].rolling(f\"{window}min\").mean()\n",
    "    df_biosignals[f'steps_min_{window_str}'] = df_biosignals['value_steps'].rolling(f\"{window}min\").min()\n",
    "    df_biosignals[f'steps_max_{window_str}'] = df_biosignals['value_steps'].rolling(f\"{window}min\").max()\n",
    "    df_biosignals[f'steps_std_{window_str}'] = df_biosignals['value_steps'].rolling(f\"{window}min\").std()\n",
    "    df_biosignals[f'steps_diff_{window_str}'] = df_biosignals[f'steps_max_{window_str}'] - df_biosignals[f'steps_min_{window_str}']\n",
    "\n",
    "# Reset index after rolling computation\n",
    "df_biosignals.reset_index(inplace=True)\n",
    "\n",
    "# ðŸ“Œ **Step 8: Merge BP Data with HR & Steps Features**\n",
    "df_merged = pd.merge_asof(df_bp_sorted, df_biosignals, left_on='datetime_local', right_on='time', direction='backward')\n",
    "\n",
    "# ðŸ“Œ **Step 9: Incorporate Stress Data (Â±15 minutes window)**\n",
    "def extract_stress_features(bp_time, df_stress):\n",
    "    start_time = bp_time - pd.Timedelta(minutes=15)\n",
    "    end_time = bp_time + pd.Timedelta(minutes=15)\n",
    "    stress_values = df_stress[(df_stress['local_created_at'] >= start_time) & (df_stress['local_created_at'] <= end_time)]['stressLevel_value']\n",
    "    return pd.Series({\n",
    "        'stress_mean': stress_values.mean(),\n",
    "        'stress_min': stress_values.min(),\n",
    "        'stress_max': stress_values.max(),\n",
    "        'stress_std': stress_values.std()\n",
    "    })\n",
    "\n",
    "df_stress_features = df_bp_sorted['datetime_local'].apply(lambda x: extract_stress_features(x, df_stress_sorted))\n",
    "df_merged = pd.concat([df_merged, df_stress_features], axis=1)\n",
    "\n",
    "# ðŸ“Œ **Step 10: Create Additional Engineered Features**\n",
    "\n",
    "# âœ… Lagged Features: only using previous data (based on BP_spike_mean and other past features)\n",
    "lag_features = ['stress_mean', 'BP_spike_mean', 'hr_mean_5min', 'steps_total_10min']\n",
    "for feature in lag_features:\n",
    "    for lag in [1, 3, 5]:\n",
    "        df_merged[f'{feature}_lag_{lag}'] = df_merged[feature].shift(lag)\n",
    "\n",
    "# âœ… Feature Interactions\n",
    "df_merged['hr_steps_ratio'] = df_merged['hr_mean_5min'] / (df_merged['steps_total_10min'] + 1)\n",
    "df_merged['stress_weighted_hr'] = df_merged['hr_mean_5min'] * df_merged['stress_mean']\n",
    "df_merged['stress_steps_ratio'] = df_merged['stress_mean'] / (df_merged['steps_total_10min'] + 1)\n",
    "df_merged['steps_hr_variability_ratio'] = df_merged['steps_std_10min'] / (df_merged['hr_std_10min'] + 1e-5)\n",
    "\n",
    "# âœ… Rolling Aggregations\n",
    "df_merged['hr_mean_rolling_3'] = df_merged['hr_mean_5min'].rolling(3).mean()\n",
    "df_merged['steps_total_rolling_5'] = df_merged['steps_total_10min'].rolling(5).mean()\n",
    "df_merged['hr_std_rolling_3'] = df_merged['hr_std_10min'].rolling(3).std()\n",
    "df_merged['cumulative_stress_30min'] = df_merged['stress_mean'].rolling(3).sum()\n",
    "df_merged['cumulative_steps_30min'] = df_merged['steps_total_10min'].rolling(3).sum()\n",
    "\n",
    "# âœ… Contextual Features\n",
    "df_merged['hour_of_day'] = df_merged['datetime_local'].dt.hour\n",
    "df_merged['day_of_week'] = df_merged['datetime_local'].dt.dayofweek\n",
    "df_merged['is_working_hours'] = df_merged['hour_of_day'].between(9, 17).astype(int)\n",
    "df_merged['is_weekend'] = (df_merged['day_of_week'] >= 5).astype(int)\n",
    "\n",
    "# âœ… Time Since Last BP Spike (based on BP_spike_mean)\n",
    "# Use the previous row's timestamp if it was a spike.\n",
    "df_merged['last_spike_time'] = df_merged['datetime_local'].shift(1).where(df_merged['BP_spike_mean'].shift(1) == 1)\n",
    "df_merged['last_spike_time'] = df_merged['last_spike_time'].ffill()\n",
    "df_merged['time_since_last_BP_spike'] = (df_merged['datetime_local'] - df_merged['last_spike_time']).dt.total_seconds() / 60\n",
    "df_merged.drop(columns=['last_spike_time'], inplace=True)\n",
    "\n",
    "# âœ… Drop Direct Current Row Measurements to Prevent Data Leakage\n",
    "drop_cols = ['systolic', 'diastolic', 'BP_spike_threshold', 'systolic_tertile', 'diastolic_tertile']\n",
    "df_merged.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "# ðŸ“Œ **Step 11: Handle Missing Values**\n",
    "df_merged.ffill(inplace=True)\n",
    "df_merged.bfill(inplace=True)  # Fixes rolling feature NaNs at the beginning\n",
    "\n",
    "# ðŸ“Œ **Step 12: Save Processed Dataset**\n",
    "df_merged.to_csv(\"processed_bp_prediction_data_full.csv\", index=False)\n",
    "print(\"âœ… Final dataset saved as 'processed_bp_prediction_data_full.csv'.\")\n",
    "print(df_merged.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4853cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ BP spike counts (train/test): 68/108  â€”  14/28\n",
      "ðŸ”¹ scale_pos_weight = 0.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "C:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "C:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "C:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "C:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "C:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Best XGB params: {'xgb__learning_rate': 0.01, 'xgb__max_depth': 5, 'xgb__n_estimators': 150, 'xgb__scale_pos_weight': 0.5882352941176471}\n",
      "ðŸ”¹ Class weights: {0.0: 1.35, 1.0: 0.7941176470588235}\n",
      "WARNING:tensorflow:From C:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "256               |256               |l1\n",
      "0.4               |0.4               |drop\n",
      "48                |48                |l2\n",
      "self              |self              |attention\n",
      "32                |32                |dense\n",
      "0.0005            |0.0005            |lr\n",
      "\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:From C:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "4/4 [==============================] - 13s 942ms/step - loss: nan - auc: 0.5000 - val_loss: nan - val_auc: 0.5000\n",
      "Epoch 2/50\n",
      "4/4 [==============================] - 1s 243ms/step - loss: nan - auc: 0.5000 - val_loss: nan - val_auc: 0.5000\n",
      "Epoch 3/50\n",
      "4/4 [==============================] - 1s 306ms/step - loss: nan - auc: 0.5000 - val_loss: nan - val_auc: 0.5000\n",
      "Epoch 4/50\n",
      "4/4 [==============================] - 1s 284ms/step - loss: nan - auc: 0.5000 - val_loss: nan - val_auc: 0.5000\n",
      "Epoch 5/50\n",
      "4/4 [==============================] - 1s 261ms/step - loss: nan - auc: 0.5000 - val_loss: nan - val_auc: 0.5000\n",
      "Epoch 6/50\n",
      "4/4 [==============================] - 1s 308ms/step - loss: nan - auc: 0.5000 - val_loss: nan - val_auc: 0.5000\n",
      "Epoch 7/50\n",
      "4/4 [==============================] - 1s 255ms/step - loss: nan - auc: 0.5000 - val_loss: nan - val_auc: 0.5000\n",
      "Epoch 8/50\n",
      "4/4 [==============================] - 1s 269ms/step - loss: nan - auc: 0.5000 - val_loss: nan - val_auc: 0.5000\n",
      "Epoch 9/50\n",
      "4/4 [==============================] - 1s 270ms/step - loss: nan - auc: 0.5000 - val_loss: nan - val_auc: 0.5000\n",
      "Epoch 10/50\n",
      "2/4 [==============>...............] - ETA: 0s - loss: nan - auc: 0.5000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 234\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m    227\u001b[0m tuner \u001b[38;5;241m=\u001b[39m kt\u001b[38;5;241m.\u001b[39mRandomSearch(\n\u001b[0;32m    228\u001b[0m     build_model,\n\u001b[0;32m    229\u001b[0m     objective\u001b[38;5;241m=\u001b[39mkt\u001b[38;5;241m.\u001b[39mObjective(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_auc\u001b[39m\u001b[38;5;124m\"\u001b[39m, direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    230\u001b[0m     max_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlstm_tuner\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    231\u001b[0m     project_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbp_spike_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m, overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    232\u001b[0m )\n\u001b[1;32m--> 234\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_lstm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m             \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m             \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test_lstm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m             \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_wt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m best_lstm \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mget_best_models(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ”¹ Best LSTM HP:\u001b[39m\u001b[38;5;124m\"\u001b[39m, tuner\u001b[38;5;241m.\u001b[39mget_best_hyperparameters(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:234\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_end(trial)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:274\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    275\u001b[0m         trial\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m trial_module\u001b[38;5;241m.\u001b[39mTrialStatus\u001b[38;5;241m.\u001b[39mCOMPLETED\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:239\u001b[0m, in \u001b[0;36mBaseTuner._run_and_update_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[1;32m--> 239\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_trial(trial\u001b[38;5;241m.\u001b[39mtrial_id)\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mexists(\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mobjective\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m    242\u001b[0m     ):\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;66;03m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;66;03m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;66;03m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[0;32m    246\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe use case of calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    254\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    255\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:314\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    312\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(model_checkpoint)\n\u001b[0;32m    313\u001b[0m     copied_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m callbacks\n\u001b[1;32m--> 314\u001b[0m     obj_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_and_fit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcopied_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    316\u001b[0m     histories\u001b[38;5;241m.\u001b[39mappend(obj_value)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m histories\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:233\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m hp \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mhyperparameters\n\u001b[0;32m    232\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_build(hp)\n\u001b[1;32m--> 233\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhypermodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;66;03m# Save the build config for model loading later.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmulti_backend():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py:149\u001b[0m, in \u001b[0;36mHyperModel.fit\u001b[1;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train the model.\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# Bloodâ€‘pressure spike prediction endâ€‘toâ€‘end\n",
    "# (XGBoost + LSTM ensemble, no oversampling)\n",
    "# =============================================\n",
    "import time, random, os\n",
    "start_time = time.time()\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility\n",
    "# -------------------------\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"42\"\n",
    "\n",
    "# -------------------------\n",
    "# Core libraries\n",
    "# -------------------------\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_tuner as kt            # â† updated import\n",
    "\n",
    "# -------------------------\n",
    "# Keras / TF layers\n",
    "# -------------------------\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM, Dense, Dropout, BatchNormalization, Bidirectional,\n",
    "    GlobalAveragePooling1D\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# =============================================\n",
    "# 1.Â DataÂ loadÂ &Â featureÂ list\n",
    "# =============================================\n",
    "df = pd.read_csv(\"processed_bp_prediction_data_full.csv\")\n",
    "\n",
    "target = \"BP_spike_mean\"\n",
    "\n",
    "features = [\n",
    "    # --- rolling stats (abbreviated comment) -------------\n",
    "    'hr_mean_5min', 'hr_min_5min', 'hr_max_5min', 'hr_std_5min',\n",
    "    'steps_total_5min', 'steps_mean_5min', 'steps_min_5min',\n",
    "    'steps_max_5min', 'steps_std_5min', 'steps_diff_5min',\n",
    "    'hr_mean_10min', 'hr_min_10min', 'hr_max_10min', 'hr_std_10min',\n",
    "    'steps_total_10min', 'steps_mean_10min', 'steps_min_10min',\n",
    "    'steps_max_10min', 'steps_std_10min', 'steps_diff_10min',\n",
    "    'hr_mean_30min', 'hr_min_30min', 'hr_max_30min', 'hr_std_30min',\n",
    "    'steps_total_30min', 'steps_mean_30min', 'steps_min_30min',\n",
    "    'steps_max_30min', 'steps_std_30min', 'steps_diff_30min',\n",
    "    'hr_mean_60min', 'hr_min_60min', 'hr_max_60min', 'hr_std_60min',\n",
    "    'steps_total_60min', 'steps_mean_60min', 'steps_min_60min',\n",
    "    'steps_max_60min', 'steps_std_60min', 'steps_diff_60min',\n",
    "    # --- stress & lags -----------------------------------\n",
    "    'stress_mean', 'stress_min', 'stress_max', 'stress_std',\n",
    "    'stress_mean_lag_1', 'stress_mean_lag_3', 'stress_mean_lag_5',\n",
    "    'BP_spike_mean_lag_1', 'BP_spike_mean_lag_3', 'BP_spike_mean_lag_5',\n",
    "    'hr_mean_5min_lag_1', 'hr_mean_5min_lag_3', 'hr_mean_5min_lag_5',\n",
    "    'steps_total_10min_lag_1', 'steps_total_10min_lag_3',\n",
    "    'steps_total_10min_lag_5',\n",
    "    # --- interactions & context --------------------------\n",
    "    'hr_steps_ratio', 'stress_weighted_hr', 'stress_steps_ratio',\n",
    "    'steps_hr_variability_ratio', 'hr_mean_rolling_3',\n",
    "    'steps_total_rolling_5', 'hr_std_rolling_3',\n",
    "    'cumulative_stress_30min', 'cumulative_steps_30min',\n",
    "    'hour_of_day', 'day_of_week', 'is_working_hours', 'is_weekend',\n",
    "    'time_since_last_BP_spike'\n",
    "]\n",
    "\n",
    "df = df[[\"datetime_local\"] + features + [target]]\n",
    "df[features] = df[features].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "df[\"datetime_local\"] = pd.to_datetime(df[\"datetime_local\"])\n",
    "train_cutoff = df[\"datetime_local\"].min() + pd.Timedelta(days=20)\n",
    "train_data = df[df[\"datetime_local\"] < train_cutoff]\n",
    "test_data  = df[df[\"datetime_local\"] >= train_cutoff]\n",
    "\n",
    "X_train, y_train = train_data[features], train_data[target]\n",
    "X_test,  y_test  = test_data[features],  test_data[target]\n",
    "\n",
    "print(\"ðŸ”¹ BP spike counts (train/test): \"\n",
    "      f\"{int(y_train.sum())}/{len(y_train)}  â€”  \"\n",
    "      f\"{int(y_test.sum())}/{len(y_test)}\")\n",
    "\n",
    "# =============================================\n",
    "# 2.Â XGBoostÂ pipelineÂ &Â gridÂ search\n",
    "# =============================================\n",
    "pos, neg = y_train.sum(), len(y_train) - y_train.sum()\n",
    "scale_pos_weight = neg / pos\n",
    "print(f\"ðŸ”¹ scale_pos_weight = {scale_pos_weight:.2f}\")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"xgb\", xgb.XGBClassifier(\n",
    "        random_state=42,\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"auc\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"xgb__max_depth\":      [3, 5, 7],\n",
    "    \"xgb__learning_rate\":  [0.01, 0.05, 0.1],\n",
    "    \"xgb__n_estimators\":   [100, 150, 200],\n",
    "    \"xgb__scale_pos_weight\": [scale_pos_weight]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    error_score=\"raise\"        # debugâ€‘friendly\n",
    ")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_xgb = grid.best_estimator_\n",
    "print(\"ðŸ”¹ Best XGB params:\", grid.best_params_)\n",
    "\n",
    "# =============================================\n",
    "# 3.Â LSTMÂ (tuned with Kerasâ€‘Tuner)\n",
    "# =============================================\n",
    "scaler_lstm = StandardScaler()\n",
    "X_train_s = scaler_lstm.fit_transform(X_train)\n",
    "X_test_s  = scaler_lstm.transform(X_test)\n",
    "\n",
    "X_train_lstm = X_train_s.reshape((X_train_s.shape[0], X_train_s.shape[1], 1))\n",
    "X_test_lstm  = X_test_s.reshape((X_test_s.shape[0],  X_test_s.shape[1],  1))\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "weights  = compute_class_weight(\"balanced\", classes=classes, y=y_train)\n",
    "class_wt = {cls: w for cls, w in zip(classes, weights)}\n",
    "print(\"ðŸ”¹ Class weights:\", class_wt)\n",
    "\n",
    "# ----- custom attention helpers -----\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(\"W\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(\"b\", shape=(input_shape[1], 1),\n",
    "                                 initializer=\"zeros\", trainable=True)\n",
    "    def call(self, x):\n",
    "        e = tf.math.tanh(tf.matmul(x, self.W) + self.b)\n",
    "        a = tf.nn.softmax(e, axis=1)\n",
    "        return tf.reduce_sum(x * a, axis=1)\n",
    "\n",
    "class MultiHeadAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, key_dim):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=key_dim)\n",
    "    def call(self, x): return tf.reduce_mean(\n",
    "        self.mha(query=x, key=x, value=x), axis=1)\n",
    "\n",
    "class SelfAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self): super().__init__(); self.att = tf.keras.layers.Attention()\n",
    "    def call(self, x): return tf.reduce_mean(self.att([x, x]), axis=1)\n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, heads, kdim, ffdim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.mha   = tf.keras.layers.MultiHeadAttention(num_heads=heads, key_dim=kdim)\n",
    "        self.ffn   = Sequential([Dense(ffdim, activation=\"relu\"), Dense(kdim)])\n",
    "        self.ln1   = tf.keras.layers.LayerNormalization()\n",
    "        self.ln2   = tf.keras.layers.LayerNormalization()\n",
    "        self.do1   = Dropout(rate); self.do2 = Dropout(rate)\n",
    "    def call(self, x):\n",
    "        attn  = self.do1(self.mha(x, x, x))\n",
    "        out1  = self.ln1(x + attn)\n",
    "        ffn   = self.do2(self.ffn(out1))\n",
    "        return self.ln2(out1 + ffn)\n",
    "\n",
    "# ----- model builder for tuner -----\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(hp.Int(\"l1\", 64, 256, 32),\n",
    "                                 return_sequences=True),\n",
    "                            input_shape=(X_train_lstm.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    dr = hp.Float(\"drop\", 0.2, 0.5, 0.1)\n",
    "    model.add(Dropout(dr))\n",
    "\n",
    "    model.add(LSTM(hp.Int(\"l2\", 32, 128, 16), return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dr))\n",
    "\n",
    "    variant = hp.Choice(\"attention\", [\"custom\", \"multi\", \"self\", \"trans\"])\n",
    "    if variant == \"custom\":\n",
    "        model.add(AttentionLayer())\n",
    "    elif variant == \"multi\":\n",
    "        model.add(MultiHeadAttentionLayer(\n",
    "            num_heads=hp.Int(\"heads\", 1, 4, 1),\n",
    "            key_dim=hp.Int(\"kdim\", 16, 64, 16)))\n",
    "    elif variant == \"self\":\n",
    "        model.add(SelfAttentionLayer())\n",
    "    else:\n",
    "        model.add(TransformerBlock(\n",
    "            heads=hp.Int(\"heads_t\", 1, 4, 1),\n",
    "            kdim=hp.Int(\"kdim_t\", 16, 64, 16),\n",
    "            ffdim=hp.Int(\"ff\", 32, 128, 32),\n",
    "            rate=dr))\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "\n",
    "    model.add(Dense(hp.Int(\"dense\", 16, 64, 16),\n",
    "                    activation=\"relu\",\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(\n",
    "                        hp.Choice(\"l2\", [0.0, 0.001, 0.01, 0.1]))))\n",
    "    model.add(Dropout(dr))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(hp.Choice(\"lr\", [1e-3, 5e-4, 1e-4])),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[tf.keras.metrics.AUC(name=\"auc\")])\n",
    "    return model\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective=kt.Objective(\"val_auc\", direction=\"max\"),\n",
    "    max_trials=20, directory=\"lstm_tuner\",\n",
    "    project_name=\"bp_spike_pred\", overwrite=True\n",
    ")\n",
    "\n",
    "tuner.search(X_train_lstm, y_train,\n",
    "             epochs=50, batch_size=32,\n",
    "             validation_data=(X_test_lstm, y_test),\n",
    "             class_weight=class_wt)\n",
    "\n",
    "best_lstm = tuner.get_best_models(1)[0]\n",
    "print(\"ðŸ”¹ Best LSTM HP:\", tuner.get_best_hyperparameters(1)[0].values)\n",
    "\n",
    "# =============================================\n",
    "# 4.Â EnsembleÂ &Â threshold search\n",
    "# =============================================\n",
    "y_xgb  = best_xgb.predict_proba(X_test)[:, 1]\n",
    "y_lstm = best_lstm.predict(X_test_lstm).ravel()\n",
    "\n",
    "alphas = np.linspace(0, 1, 11)\n",
    "best_auc, best_alpha = -1, None\n",
    "for a in alphas:\n",
    "    auc = roc_auc_score(y_test, a*y_xgb + (1-a)*y_lstm)\n",
    "    if auc > best_auc: best_auc, best_alpha = auc, a\n",
    "print(f\"ðŸ”¹ Ensemble AUROC = {best_auc:.3f}  (Î±={best_alpha:.2f})\")\n",
    "best_beta = 1 - best_alpha\n",
    "\n",
    "best_thr, best_youden = None, -1\n",
    "for thr in np.arange(0, 1.01, 0.01):\n",
    "    y_bin = ((best_alpha*y_xgb + best_beta*y_lstm) >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_bin).ravel()\n",
    "    sens = tp/(tp+fn) if tp+fn else 0\n",
    "    spec = tn/(tn+fp) if tn+fp else 0\n",
    "    ydn  = sens + spec - 1\n",
    "    if ydn > best_youden: best_thr, best_youden, best_sens, best_spec = thr, ydn, sens, spec\n",
    "print(f\"ðŸ”¹ Optimal threshold = {best_thr:.2f} (sens={best_sens:.2f}, spec={best_spec:.2f})\")\n",
    "\n",
    "# =============================================\n",
    "# 5.Â Sensitivityâ€‘specificity plot\n",
    "# =============================================\n",
    "thr = np.arange(0, 1.01, 0.01)\n",
    "sens, spec = [], []\n",
    "for t in thr:\n",
    "    yb = ((best_alpha*y_xgb + best_beta*y_lstm) >= t).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, yb).ravel()\n",
    "    sens.append(tp/(tp+fn) if tp+fn else 0)\n",
    "    spec.append(tn/(tn+fp) if tn+fp else 0)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(thr, sens, \"-o\", label=\"Sensitivity\")\n",
    "plt.plot(thr, spec, \"-s\", label=\"Specificity\")\n",
    "plt.xlabel(\"Decision threshold\"); plt.ylabel(\"Value\")\n",
    "plt.title(f\"TPR / TNR tradeâ€‘off â€” AUROCÂ {best_auc:.3f}\")\n",
    "plt.grid(); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# =============================================\n",
    "# 6.Â SHAP forÂ XGBoost\n",
    "# =============================================\n",
    "explainer  = shap.Explainer(best_xgb.named_steps[\"xgb\"])\n",
    "shap_vals  = explainer(best_xgb.named_steps[\"scaler\"].transform(X_test))\n",
    "shap.summary_plot(shap_vals, X_test, feature_names=features)\n",
    "\n",
    "# =============================================\n",
    "# 7.Â TimeÂ taken\n",
    "# =============================================\n",
    "print(f\"\\nTotal run time: {time.time()-start_time:.1f}Â s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0183d5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ BP Spike Counts Before Resampling:\n",
      "   - Training Set: 127.0 spikes out of 177 samples (71.75%)\n",
      "   - Test Set: 27.0 spikes out of 43 samples (62.79%)\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ”¹ BP Spike Counts Before Resampling:\")\n",
    "print(f\"   - Training Set: {sum(y_train)} spikes out of {len(y_train)} samples ({sum(y_train)/len(y_train)*100:.2f}%)\")\n",
    "print(f\"   - Test Set: {sum(y_test)} spikes out of {len(y_test)} samples ({sum(y_test)/len(y_test)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d2776f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
