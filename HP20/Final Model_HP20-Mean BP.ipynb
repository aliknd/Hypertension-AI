{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45a18412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  time data_type  value\n",
      "0  2024-07-15 11:19:00        hr     70\n",
      "1  2024-07-15 11:20:00        hr     72\n",
      "2  2024-07-15 11:26:00        hr     74\n",
      "3  2024-07-15 11:27:00        hr     72\n",
      "4  2024-07-15 11:28:00        hr     95\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"hp20_hr.csv\"  # Replace with your actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert the time column to datetime format and remove timezone information\n",
    "df['time'] = pd.to_datetime(df['time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Sort the dataframe by time in increasing order\n",
    "df_sorted = df.sort_values(by='time')\n",
    "\n",
    "# Display the first few rows of the sorted dataframe\n",
    "print(df_sorted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02f17bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  time data_type  value\n",
      "0  2024-07-15 11:40:00     steps      9\n",
      "1  2024-07-15 11:45:00     steps     63\n",
      "2  2024-07-15 11:46:00     steps     55\n",
      "3  2024-07-15 11:47:00     steps     92\n",
      "4  2024-07-15 11:48:00     steps     97\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the steps dataset\n",
    "file_path_steps = \"hp20_steps.csv\"  # Replace with your actual file path\n",
    "df_steps = pd.read_csv(file_path_steps)\n",
    "\n",
    "# Convert the time column to datetime format and remove timezone information\n",
    "df_steps['time'] = pd.to_datetime(df_steps['time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Sort the dataframe by time in increasing order\n",
    "df_steps_sorted = df_steps.sort_values(by='time')\n",
    "\n",
    "# Display the first few rows of the sorted dataframe\n",
    "print(df_steps_sorted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bac0d4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 220\n",
      "Number of BP spikes: 162\n",
      "       datetime_local  systolic  diastolic  BP_spike\n",
      "0 2024-07-15 11:24:39       142         90         1\n",
      "1 2024-07-15 12:34:22       138         91         1\n",
      "2 2024-07-15 13:56:53       125         87         1\n",
      "3 2024-07-15 15:07:37       134         97         1\n",
      "9 2024-07-15 17:18:51       128         85         1\n"
     ]
    }
   ],
   "source": [
    "# Load the blood pressure dataset\n",
    "file_path_bp = \"blood_pressure_readings_ID20_cleaned.csv\"  # Replace with your actual file path\n",
    "df_bp = pd.read_csv(file_path_bp)\n",
    "\n",
    "# Select relevant columns\n",
    "df_bp = df_bp[['datetime_local', 'systolic', 'diastolic']]\n",
    "\n",
    "# Convert the datetime column to datetime format\n",
    "df_bp['datetime_local'] = pd.to_datetime(df_bp['datetime_local'])\n",
    "\n",
    "# Sort the dataframe by datetime in increasing order\n",
    "df_bp_sorted = df_bp.sort_values(by='datetime_local')\n",
    "\n",
    "# Add a binary classification column for BP spikes\n",
    "df_bp_sorted['BP_spike'] = ((df_bp_sorted['systolic'] > 130) | (df_bp_sorted['diastolic'] > 80)).astype(int)\n",
    "\n",
    "# Count the number of BP spikes and total records\n",
    "total_records = len(df_bp_sorted)\n",
    "bp_spike_count = df_bp_sorted['BP_spike'].sum()\n",
    "\n",
    "# Print summary\n",
    "print(f\"Total records: {total_records}\")\n",
    "print(f\"Number of BP spikes: {bp_spike_count}\")\n",
    "\n",
    "# Display the first few rows of the processed dataframe\n",
    "print(df_bp_sorted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "554387f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     local_created_at  stressLevel_value\n",
      "0 2024-07-15 11:34:00                  4\n",
      "1 2024-07-15 12:33:00                  5\n",
      "2 2024-07-15 13:55:00                  3\n",
      "3 2024-07-15 15:06:00                  2\n",
      "4 2024-07-15 17:17:00                  4\n"
     ]
    }
   ],
   "source": [
    "# Load the stress data dataset\n",
    "file_path_stress = \"questionnaire_responses_ID20.csv\"  # Replace with your actual file path\n",
    "df_stress = pd.read_csv(file_path_stress)\n",
    "\n",
    "# Select relevant columns\n",
    "df_stress = df_stress[['local_created_at', 'stressLevel_value']]\n",
    "\n",
    "# Convert the time column to datetime format\n",
    "df_stress['local_created_at'] = pd.to_datetime(df_stress['local_created_at'])\n",
    "\n",
    "# Sort the dataframe by time in increasing order\n",
    "df_stress_sorted = df_stress.sort_values(by='local_created_at')\n",
    "\n",
    "# Display the first few rows of the processed dataframe\n",
    "print(df_stress_sorted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bbcb8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final dataset saved as 'processed_bp_prediction_data_full.csv'.\n",
      "     id  user_id     reading_id             datetime      datetime_local  \\\n",
      "0  1025       20  1721078679000  2024-07-15 21:24:39 2024-07-15 11:24:39   \n",
      "1  1026       20  1721082862000  2024-07-15 22:34:22 2024-07-15 12:34:22   \n",
      "2  1027       20  1721087813000  2024-07-15 23:56:53 2024-07-15 13:56:53   \n",
      "3  1028       20  1721092057000  2024-07-16 01:07:37 2024-07-15 15:07:37   \n",
      "4  1034       20  1721099931000  2024-07-16 03:18:51 2024-07-15 17:18:51   \n",
      "\n",
      "   pulse  device_type           created_at  BP_spike_mean                time  \\\n",
      "0     88  HeartGuide™  2024-07-15 21:39:20              1 2024-07-15 11:20:00   \n",
      "1     88  HeartGuide™  2024-07-15 22:39:21              1 2024-07-15 12:34:00   \n",
      "2     78  HeartGuide™  2024-07-16 00:04:22              1 2024-07-15 13:56:00   \n",
      "3     75  HeartGuide™  2024-07-16 01:14:18              1 2024-07-15 15:07:00   \n",
      "4     61  HeartGuide™  2024-07-16 19:24:19              1 2024-07-15 17:18:00   \n",
      "\n",
      "   ... hr_mean_rolling_3  steps_total_rolling_5 hr_std_rolling_3  \\\n",
      "0  ...         85.266667                   95.4         3.196794   \n",
      "1  ...         85.266667                   95.4         3.196794   \n",
      "2  ...         85.266667                   95.4         3.196794   \n",
      "3  ...         88.066667                   95.4         2.805139   \n",
      "4  ...         83.933333                   95.4         2.756282   \n",
      "\n",
      "   cumulative_stress_30min  cumulative_steps_30min  hour_of_day  day_of_week  \\\n",
      "0                     12.0                   130.0           11            0   \n",
      "1                     12.0                   130.0           12            0   \n",
      "2                     12.0                   130.0           13            0   \n",
      "3                     10.0                   130.0           15            0   \n",
      "4                      9.0                   173.0           17            0   \n",
      "\n",
      "   is_working_hours  is_weekend  time_since_last_BP_spike  \n",
      "0                 1           0                 69.716667  \n",
      "1                 1           0                 69.716667  \n",
      "2                 1           0                 82.516667  \n",
      "3                 1           0                 70.733333  \n",
      "4                 1           0                131.233333  \n",
      "\n",
      "[5 rows x 84 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 📌 **Step 1: Load Datasets**\n",
    "file_path_hr = \"hp20_hr.csv\"\n",
    "file_path_steps = \"hp20_steps.csv\"\n",
    "file_path_bp = \"blood_pressure_readings_ID20_cleaned.csv\"\n",
    "file_path_stress = \"questionnaire_responses_ID20.csv\"\n",
    "\n",
    "df_hr = pd.read_csv(file_path_hr)\n",
    "df_steps = pd.read_csv(file_path_steps)\n",
    "df_bp = pd.read_csv(file_path_bp)\n",
    "df_stress = pd.read_csv(file_path_stress)\n",
    "\n",
    "# 📌 **Step 2: Convert timestamps to uniform `datetime64[ns]`**\n",
    "df_hr['time'] = pd.to_datetime(df_hr['time']).dt.tz_localize(None)\n",
    "df_steps['time'] = pd.to_datetime(df_steps['time']).dt.tz_localize(None)\n",
    "df_bp['datetime_local'] = pd.to_datetime(df_bp['datetime_local']).dt.tz_localize(None)\n",
    "df_stress['local_created_at'] = pd.to_datetime(df_stress['local_created_at']).dt.tz_localize(None)\n",
    "\n",
    "# 📌 **Step 3: Remove Data from `2024-10-23` for BP & Stress**\n",
    "df_bp = df_bp[df_bp['datetime_local'].dt.date > pd.to_datetime(\"2024-07-14\").date()]\n",
    "df_stress = df_stress[df_stress['local_created_at'].dt.date > pd.to_datetime(\"2024-07-14\").date()]\n",
    "\n",
    "# 📌 **Step 4: Sort datasets by time**\n",
    "df_hr_sorted = df_hr.sort_values(by='time')\n",
    "df_steps_sorted = df_steps.sort_values(by='time')\n",
    "df_bp_sorted = df_bp.sort_values(by='datetime_local')\n",
    "df_stress_sorted = df_stress.sort_values(by='local_created_at')\n",
    "\n",
    "# 📌 **Step 5: Add BP spike classification**\n",
    "# Retain the threshold-based definition for reference:\n",
    "df_bp_sorted['BP_spike_threshold'] = ((df_bp_sorted['systolic'] > 130) | (df_bp_sorted['diastolic'] > 80)).astype(int)\n",
    "\n",
    "# New binary definition based on mean values:\n",
    "mean_systolic = df_bp_sorted['systolic'].mean()\n",
    "mean_diastolic = df_bp_sorted['diastolic'].mean()\n",
    "df_bp_sorted['BP_spike_mean'] = ((df_bp_sorted['systolic'] > mean_systolic) | (df_bp_sorted['diastolic'] > mean_diastolic)).astype(int)\n",
    "\n",
    "# Tertile classification for systolic and diastolic:\n",
    "systolic_lower = df_bp_sorted['systolic'].quantile(0.333)\n",
    "systolic_upper = df_bp_sorted['systolic'].quantile(0.667)\n",
    "diastolic_lower = df_bp_sorted['diastolic'].quantile(0.333)\n",
    "diastolic_upper = df_bp_sorted['diastolic'].quantile(0.667)\n",
    "\n",
    "def classify_bp(value, lower, upper):\n",
    "    if value < lower:\n",
    "        return \"low\"\n",
    "    elif value < upper:\n",
    "        return \"mid\"\n",
    "    else:\n",
    "        return \"high\"\n",
    "\n",
    "df_bp_sorted['systolic_tertile'] = df_bp_sorted['systolic'].apply(lambda x: classify_bp(x, systolic_lower, systolic_upper))\n",
    "df_bp_sorted['diastolic_tertile'] = df_bp_sorted['diastolic'].apply(lambda x: classify_bp(x, diastolic_lower, diastolic_upper))\n",
    "\n",
    "# 📌 **Step 6: Merge HR & Steps Data Based on Nearest Timestamp**\n",
    "df_biosignals = pd.merge_asof(df_hr_sorted, df_steps_sorted, on='time', direction='backward', suffixes=('_hr', '_steps'))\n",
    "\n",
    "# 📌 **Step 7: Compute Rolling Window Statistics for HR & Steps**\n",
    "df_biosignals.set_index('time', inplace=True)\n",
    "time_windows = [5, 10, 30, 60]  # Define time windows (minutes)\n",
    "\n",
    "for window in time_windows:\n",
    "    window_str = f\"{window}min\"\n",
    "    # Rolling windows here include the current row; use .shift(1) later if strictly previous data is needed.\n",
    "    df_biosignals[f'hr_mean_{window_str}'] = df_biosignals['value_hr'].rolling(f\"{window}min\").mean()\n",
    "    df_biosignals[f'hr_min_{window_str}'] = df_biosignals['value_hr'].rolling(f\"{window}min\").min()\n",
    "    df_biosignals[f'hr_max_{window_str}'] = df_biosignals['value_hr'].rolling(f\"{window}min\").max()\n",
    "    df_biosignals[f'hr_std_{window_str}'] = df_biosignals['value_hr'].rolling(f\"{window}min\").std()\n",
    "\n",
    "    df_biosignals[f'steps_total_{window_str}'] = df_biosignals['value_steps'].rolling(f\"{window}min\").sum()\n",
    "    df_biosignals[f'steps_mean_{window_str}'] = df_biosignals['value_steps'].rolling(f\"{window}min\").mean()\n",
    "    df_biosignals[f'steps_min_{window_str}'] = df_biosignals['value_steps'].rolling(f\"{window}min\").min()\n",
    "    df_biosignals[f'steps_max_{window_str}'] = df_biosignals['value_steps'].rolling(f\"{window}min\").max()\n",
    "    df_biosignals[f'steps_std_{window_str}'] = df_biosignals['value_steps'].rolling(f\"{window}min\").std()\n",
    "    df_biosignals[f'steps_diff_{window_str}'] = df_biosignals[f'steps_max_{window_str}'] - df_biosignals[f'steps_min_{window_str}']\n",
    "\n",
    "# Reset index after rolling computation\n",
    "df_biosignals.reset_index(inplace=True)\n",
    "\n",
    "# 📌 **Step 8: Merge BP Data with HR & Steps Features**\n",
    "df_merged = pd.merge_asof(df_bp_sorted, df_biosignals, left_on='datetime_local', right_on='time', direction='backward')\n",
    "\n",
    "# 📌 **Step 9: Incorporate Stress Data (±15 minutes window)**\n",
    "def extract_stress_features(bp_time, df_stress):\n",
    "    start_time = bp_time - pd.Timedelta(minutes=15)\n",
    "    end_time = bp_time + pd.Timedelta(minutes=15)\n",
    "    stress_values = df_stress[(df_stress['local_created_at'] >= start_time) & (df_stress['local_created_at'] <= end_time)]['stressLevel_value']\n",
    "    return pd.Series({\n",
    "        'stress_mean': stress_values.mean(),\n",
    "        'stress_min': stress_values.min(),\n",
    "        'stress_max': stress_values.max(),\n",
    "        'stress_std': stress_values.std()\n",
    "    })\n",
    "\n",
    "df_stress_features = df_bp_sorted['datetime_local'].apply(lambda x: extract_stress_features(x, df_stress_sorted))\n",
    "df_merged = pd.concat([df_merged, df_stress_features], axis=1)\n",
    "\n",
    "# 📌 **Step 10: Create Additional Engineered Features**\n",
    "\n",
    "# ✅ Lagged Features: only using previous data (based on BP_spike_mean and other past features)\n",
    "lag_features = ['stress_mean', 'BP_spike_mean', 'hr_mean_5min', 'steps_total_10min']\n",
    "for feature in lag_features:\n",
    "    for lag in [1, 3, 5]:\n",
    "        df_merged[f'{feature}_lag_{lag}'] = df_merged[feature].shift(lag)\n",
    "\n",
    "# ✅ Feature Interactions\n",
    "df_merged['hr_steps_ratio'] = df_merged['hr_mean_5min'] / (df_merged['steps_total_10min'] + 1)\n",
    "df_merged['stress_weighted_hr'] = df_merged['hr_mean_5min'] * df_merged['stress_mean']\n",
    "df_merged['stress_steps_ratio'] = df_merged['stress_mean'] / (df_merged['steps_total_10min'] + 1)\n",
    "df_merged['steps_hr_variability_ratio'] = df_merged['steps_std_10min'] / (df_merged['hr_std_10min'] + 1e-5)\n",
    "\n",
    "# ✅ Rolling Aggregations\n",
    "df_merged['hr_mean_rolling_3'] = df_merged['hr_mean_5min'].rolling(3).mean()\n",
    "df_merged['steps_total_rolling_5'] = df_merged['steps_total_10min'].rolling(5).mean()\n",
    "df_merged['hr_std_rolling_3'] = df_merged['hr_std_10min'].rolling(3).std()\n",
    "df_merged['cumulative_stress_30min'] = df_merged['stress_mean'].rolling(3).sum()\n",
    "df_merged['cumulative_steps_30min'] = df_merged['steps_total_10min'].rolling(3).sum()\n",
    "\n",
    "# ✅ Contextual Features\n",
    "df_merged['hour_of_day'] = df_merged['datetime_local'].dt.hour\n",
    "df_merged['day_of_week'] = df_merged['datetime_local'].dt.dayofweek\n",
    "df_merged['is_working_hours'] = df_merged['hour_of_day'].between(9, 17).astype(int)\n",
    "df_merged['is_weekend'] = (df_merged['day_of_week'] >= 5).astype(int)\n",
    "\n",
    "# ✅ Time Since Last BP Spike (based on BP_spike_mean)\n",
    "# Use the previous row's timestamp if it was a spike.\n",
    "df_merged['last_spike_time'] = df_merged['datetime_local'].shift(1).where(df_merged['BP_spike_mean'].shift(1) == 1)\n",
    "df_merged['last_spike_time'] = df_merged['last_spike_time'].ffill()\n",
    "df_merged['time_since_last_BP_spike'] = (df_merged['datetime_local'] - df_merged['last_spike_time']).dt.total_seconds() / 60\n",
    "df_merged.drop(columns=['last_spike_time'], inplace=True)\n",
    "\n",
    "# ✅ Drop Direct Current Row Measurements to Prevent Data Leakage\n",
    "drop_cols = ['systolic', 'diastolic', 'BP_spike_threshold', 'systolic_tertile', 'diastolic_tertile']\n",
    "df_merged.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "# 📌 **Step 11: Handle Missing Values**\n",
    "df_merged.ffill(inplace=True)\n",
    "df_merged.bfill(inplace=True)  # Fixes rolling feature NaNs at the beginning\n",
    "\n",
    "# 📌 **Step 12: Save Processed Dataset**\n",
    "df_merged.to_csv(\"processed_bp_prediction_data_full.csv\", index=False)\n",
    "print(\"✅ Final dataset saved as 'processed_bp_prediction_data_full.csv'.\")\n",
    "print(df_merged.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4853cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 BP spike counts (train/test): 131/220  —  0/0\n",
      "🔹 scale_pos_weight = 0.68\n",
      "🔹 Best XGB params: {'xgb__learning_rate': 0.1, 'xgb__max_depth': 5, 'xgb__n_estimators': 100, 'xgb__scale_pos_weight': 0.6793893129770993}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 70)) while a minimum of 1 is required by StandardScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 135\u001b[0m\n\u001b[0;32m    133\u001b[0m scaler_lstm \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m    134\u001b[0m X_train_s \u001b[38;5;241m=\u001b[39m scaler_lstm\u001b[38;5;241m.\u001b[39mfit_transform(X_train)\n\u001b[1;32m--> 135\u001b[0m X_test_s  \u001b[38;5;241m=\u001b[39m \u001b[43mscaler_lstm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m X_train_lstm \u001b[38;5;241m=\u001b[39m X_train_s\u001b[38;5;241m.\u001b[39mreshape((X_train_s\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], X_train_s\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    138\u001b[0m X_test_lstm  \u001b[38;5;241m=\u001b[39m X_test_s\u001b[38;5;241m.\u001b[39mreshape((X_test_s\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],  X_test_s\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],  \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1043\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1040\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1042\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m-> 1043\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1072\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1070\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[0;32m   1071\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[1;32m-> 1072\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1073\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1074\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1075\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m   1076\u001b[0m         )\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1079\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 70)) while a minimum of 1 is required by StandardScaler."
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# Blood‑pressure spike prediction end‑to‑end\n",
    "# (XGBoost + LSTM ensemble, no oversampling)\n",
    "# =============================================\n",
    "import time, random, os\n",
    "start_time = time.time()\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility\n",
    "# -------------------------\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"42\"\n",
    "\n",
    "# -------------------------\n",
    "# Core libraries\n",
    "# -------------------------\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_tuner as kt            # ← updated import\n",
    "\n",
    "# -------------------------\n",
    "# Keras / TF layers\n",
    "# -------------------------\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM, Dense, Dropout, BatchNormalization, Bidirectional,\n",
    "    GlobalAveragePooling1D\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# =============================================\n",
    "# 1. Data load & feature list\n",
    "# =============================================\n",
    "df = pd.read_csv(\"processed_bp_prediction_data_full.csv\")\n",
    "\n",
    "target = \"BP_spike_mean\"\n",
    "\n",
    "features = [\n",
    "    # --- rolling stats (abbreviated comment) -------------\n",
    "    'hr_mean_5min', 'hr_min_5min', 'hr_max_5min', 'hr_std_5min',\n",
    "    'steps_total_5min', 'steps_mean_5min', 'steps_min_5min',\n",
    "    'steps_max_5min', 'steps_std_5min', 'steps_diff_5min',\n",
    "    'hr_mean_10min', 'hr_min_10min', 'hr_max_10min', 'hr_std_10min',\n",
    "    'steps_total_10min', 'steps_mean_10min', 'steps_min_10min',\n",
    "    'steps_max_10min', 'steps_std_10min', 'steps_diff_10min',\n",
    "    'hr_mean_30min', 'hr_min_30min', 'hr_max_30min', 'hr_std_30min',\n",
    "    'steps_total_30min', 'steps_mean_30min', 'steps_min_30min',\n",
    "    'steps_max_30min', 'steps_std_30min', 'steps_diff_30min',\n",
    "    'hr_mean_60min', 'hr_min_60min', 'hr_max_60min', 'hr_std_60min',\n",
    "    'steps_total_60min', 'steps_mean_60min', 'steps_min_60min',\n",
    "    'steps_max_60min', 'steps_std_60min', 'steps_diff_60min',\n",
    "    # --- stress & lags -----------------------------------\n",
    "    'stress_mean', 'stress_min', 'stress_max', 'stress_std',\n",
    "    'stress_mean_lag_1', 'stress_mean_lag_3', 'stress_mean_lag_5',\n",
    "    'BP_spike_mean_lag_1', 'BP_spike_mean_lag_3', 'BP_spike_mean_lag_5',\n",
    "    'hr_mean_5min_lag_1', 'hr_mean_5min_lag_3', 'hr_mean_5min_lag_5',\n",
    "    'steps_total_10min_lag_1', 'steps_total_10min_lag_3',\n",
    "    'steps_total_10min_lag_5',\n",
    "    # --- interactions & context --------------------------\n",
    "    'hr_steps_ratio', 'stress_weighted_hr', 'stress_steps_ratio',\n",
    "    'steps_hr_variability_ratio', 'hr_mean_rolling_3',\n",
    "    'steps_total_rolling_5', 'hr_std_rolling_3',\n",
    "    'cumulative_stress_30min', 'cumulative_steps_30min',\n",
    "    'hour_of_day', 'day_of_week', 'is_working_hours', 'is_weekend',\n",
    "    'time_since_last_BP_spike'\n",
    "]\n",
    "\n",
    "df = df[[\"datetime_local\"] + features + [target]]\n",
    "df[features] = df[features].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "df[\"datetime_local\"] = pd.to_datetime(df[\"datetime_local\"])\n",
    "train_cutoff = df[\"datetime_local\"].min() + pd.Timedelta(days=20)\n",
    "train_data = df[df[\"datetime_local\"] < train_cutoff]\n",
    "test_data  = df[df[\"datetime_local\"] >= train_cutoff]\n",
    "\n",
    "X_train, y_train = train_data[features], train_data[target]\n",
    "X_test,  y_test  = test_data[features],  test_data[target]\n",
    "\n",
    "print(\"🔹 BP spike counts (train/test): \"\n",
    "      f\"{int(y_train.sum())}/{len(y_train)}  —  \"\n",
    "      f\"{int(y_test.sum())}/{len(y_test)}\")\n",
    "\n",
    "# =============================================\n",
    "# 2. XGBoost pipeline & grid search\n",
    "# =============================================\n",
    "pos, neg = y_train.sum(), len(y_train) - y_train.sum()\n",
    "scale_pos_weight = neg / pos\n",
    "print(f\"🔹 scale_pos_weight = {scale_pos_weight:.2f}\")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"xgb\", xgb.XGBClassifier(\n",
    "        random_state=42,\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"auc\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"xgb__max_depth\":      [3, 5, 7],\n",
    "    \"xgb__learning_rate\":  [0.01, 0.05, 0.1],\n",
    "    \"xgb__n_estimators\":   [100, 150, 200],\n",
    "    \"xgb__scale_pos_weight\": [scale_pos_weight]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    error_score=\"raise\"        # debug‑friendly\n",
    ")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_xgb = grid.best_estimator_\n",
    "print(\"🔹 Best XGB params:\", grid.best_params_)\n",
    "\n",
    "# =============================================\n",
    "# 3. LSTM (tuned with Keras‑Tuner)\n",
    "# =============================================\n",
    "scaler_lstm = StandardScaler()\n",
    "X_train_s = scaler_lstm.fit_transform(X_train)\n",
    "X_test_s  = scaler_lstm.transform(X_test)\n",
    "\n",
    "X_train_lstm = X_train_s.reshape((X_train_s.shape[0], X_train_s.shape[1], 1))\n",
    "X_test_lstm  = X_test_s.reshape((X_test_s.shape[0],  X_test_s.shape[1],  1))\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "weights  = compute_class_weight(\"balanced\", classes=classes, y=y_train)\n",
    "class_wt = {cls: w for cls, w in zip(classes, weights)}\n",
    "print(\"🔹 Class weights:\", class_wt)\n",
    "\n",
    "# ----- custom attention helpers -----\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(\"W\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(\"b\", shape=(input_shape[1], 1),\n",
    "                                 initializer=\"zeros\", trainable=True)\n",
    "    def call(self, x):\n",
    "        e = tf.math.tanh(tf.matmul(x, self.W) + self.b)\n",
    "        a = tf.nn.softmax(e, axis=1)\n",
    "        return tf.reduce_sum(x * a, axis=1)\n",
    "\n",
    "class MultiHeadAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, key_dim):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=key_dim)\n",
    "    def call(self, x): return tf.reduce_mean(\n",
    "        self.mha(query=x, key=x, value=x), axis=1)\n",
    "\n",
    "class SelfAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self): super().__init__(); self.att = tf.keras.layers.Attention()\n",
    "    def call(self, x): return tf.reduce_mean(self.att([x, x]), axis=1)\n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, heads, kdim, ffdim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.mha   = tf.keras.layers.MultiHeadAttention(num_heads=heads, key_dim=kdim)\n",
    "        self.ffn   = Sequential([Dense(ffdim, activation=\"relu\"), Dense(kdim)])\n",
    "        self.ln1   = tf.keras.layers.LayerNormalization()\n",
    "        self.ln2   = tf.keras.layers.LayerNormalization()\n",
    "        self.do1   = Dropout(rate); self.do2 = Dropout(rate)\n",
    "    def call(self, x):\n",
    "        attn  = self.do1(self.mha(x, x, x))\n",
    "        out1  = self.ln1(x + attn)\n",
    "        ffn   = self.do2(self.ffn(out1))\n",
    "        return self.ln2(out1 + ffn)\n",
    "\n",
    "# ----- model builder for tuner -----\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(hp.Int(\"l1\", 64, 256, 32),\n",
    "                                 return_sequences=True),\n",
    "                            input_shape=(X_train_lstm.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    dr = hp.Float(\"drop\", 0.2, 0.5, 0.1)\n",
    "    model.add(Dropout(dr))\n",
    "\n",
    "    model.add(LSTM(hp.Int(\"l2\", 32, 128, 16), return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dr))\n",
    "\n",
    "    variant = hp.Choice(\"attention\", [\"custom\", \"multi\", \"self\", \"trans\"])\n",
    "    if variant == \"custom\":\n",
    "        model.add(AttentionLayer())\n",
    "    elif variant == \"multi\":\n",
    "        model.add(MultiHeadAttentionLayer(\n",
    "            num_heads=hp.Int(\"heads\", 1, 4, 1),\n",
    "            key_dim=hp.Int(\"kdim\", 16, 64, 16)))\n",
    "    elif variant == \"self\":\n",
    "        model.add(SelfAttentionLayer())\n",
    "    else:\n",
    "        model.add(TransformerBlock(\n",
    "            heads=hp.Int(\"heads_t\", 1, 4, 1),\n",
    "            kdim=hp.Int(\"kdim_t\", 16, 64, 16),\n",
    "            ffdim=hp.Int(\"ff\", 32, 128, 32),\n",
    "            rate=dr))\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "\n",
    "    model.add(Dense(hp.Int(\"dense\", 16, 64, 16),\n",
    "                    activation=\"relu\",\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(\n",
    "                        hp.Choice(\"l2\", [0.0, 0.001, 0.01, 0.1]))))\n",
    "    model.add(Dropout(dr))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(hp.Choice(\"lr\", [1e-3, 5e-4, 1e-4])),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[tf.keras.metrics.AUC(name=\"auc\")])\n",
    "    return model\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective=kt.Objective(\"val_auc\", direction=\"max\"),\n",
    "    max_trials=20, directory=\"lstm_tuner\",\n",
    "    project_name=\"bp_spike_pred\", overwrite=True\n",
    ")\n",
    "\n",
    "tuner.search(X_train_lstm, y_train,\n",
    "             epochs=50, batch_size=32,\n",
    "             validation_data=(X_test_lstm, y_test),\n",
    "             class_weight=class_wt)\n",
    "\n",
    "best_lstm = tuner.get_best_models(1)[0]\n",
    "print(\"🔹 Best LSTM HP:\", tuner.get_best_hyperparameters(1)[0].values)\n",
    "\n",
    "# =============================================\n",
    "# 4. Ensemble & threshold search\n",
    "# =============================================\n",
    "y_xgb  = best_xgb.predict_proba(X_test)[:, 1]\n",
    "y_lstm = best_lstm.predict(X_test_lstm).ravel()\n",
    "\n",
    "alphas = np.linspace(0, 1, 11)\n",
    "best_auc, best_alpha = -1, None\n",
    "for a in alphas:\n",
    "    auc = roc_auc_score(y_test, a*y_xgb + (1-a)*y_lstm)\n",
    "    if auc > best_auc: best_auc, best_alpha = auc, a\n",
    "print(f\"🔹 Ensemble AUROC = {best_auc:.3f}  (α={best_alpha:.2f})\")\n",
    "best_beta = 1 - best_alpha\n",
    "\n",
    "best_thr, best_youden = None, -1\n",
    "for thr in np.arange(0, 1.01, 0.01):\n",
    "    y_bin = ((best_alpha*y_xgb + best_beta*y_lstm) >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_bin).ravel()\n",
    "    sens = tp/(tp+fn) if tp+fn else 0\n",
    "    spec = tn/(tn+fp) if tn+fp else 0\n",
    "    ydn  = sens + spec - 1\n",
    "    if ydn > best_youden: best_thr, best_youden, best_sens, best_spec = thr, ydn, sens, spec\n",
    "print(f\"🔹 Optimal threshold = {best_thr:.2f} (sens={best_sens:.2f}, spec={best_spec:.2f})\")\n",
    "\n",
    "# =============================================\n",
    "# 5. Sensitivity‑specificity plot\n",
    "# =============================================\n",
    "thr = np.arange(0, 1.01, 0.01)\n",
    "sens, spec = [], []\n",
    "for t in thr:\n",
    "    yb = ((best_alpha*y_xgb + best_beta*y_lstm) >= t).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, yb).ravel()\n",
    "    sens.append(tp/(tp+fn) if tp+fn else 0)\n",
    "    spec.append(tn/(tn+fp) if tn+fp else 0)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(thr, sens, \"-o\", label=\"Sensitivity\")\n",
    "plt.plot(thr, spec, \"-s\", label=\"Specificity\")\n",
    "plt.xlabel(\"Decision threshold\"); plt.ylabel(\"Value\")\n",
    "plt.title(f\"TPR / TNR trade‑off — AUROC {best_auc:.3f}\")\n",
    "plt.grid(); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# =============================================\n",
    "# 6. SHAP for XGBoost\n",
    "# =============================================\n",
    "explainer  = shap.Explainer(best_xgb.named_steps[\"xgb\"])\n",
    "shap_vals  = explainer(best_xgb.named_steps[\"scaler\"].transform(X_test))\n",
    "shap.summary_plot(shap_vals, X_test, feature_names=features)\n",
    "\n",
    "# =============================================\n",
    "# 7. Time taken\n",
    "# =============================================\n",
    "print(f\"\\nTotal run time: {time.time()-start_time:.1f} s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0183d5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 BP Spike Counts Before Resampling:\n",
      "   - Training Set: 127.0 spikes out of 177 samples (71.75%)\n",
      "   - Test Set: 27.0 spikes out of 43 samples (62.79%)\n"
     ]
    }
   ],
   "source": [
    "print(\"🔹 BP Spike Counts Before Resampling:\")\n",
    "print(f\"   - Training Set: {sum(y_train)} spikes out of {len(y_train)} samples ({sum(y_train)/len(y_train)*100:.2f}%)\")\n",
    "print(f\"   - Test Set: {sum(y_test)} spikes out of {len(y_test)} samples ({sum(y_test)/len(y_test)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d2776f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
