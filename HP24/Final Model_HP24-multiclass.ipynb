{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a18412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"hp24_hr.csv\"  # Replace with your actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert the time column to datetime format and remove timezone information\n",
    "df['time'] = pd.to_datetime(df['time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Sort the dataframe by time in increasing order\n",
    "df_sorted = df.sort_values(by='time')\n",
    "\n",
    "# Display the first few rows of the sorted dataframe\n",
    "print(df_sorted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f17bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the steps dataset\n",
    "file_path_steps = \"hp24_steps.csv\"  # Replace with your actual file path\n",
    "df_steps = pd.read_csv(file_path_steps)\n",
    "\n",
    "# Convert the time column to datetime format and remove timezone information\n",
    "df_steps['time'] = pd.to_datetime(df_steps['time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Sort the dataframe by time in increasing order\n",
    "df_steps_sorted = df_steps.sort_values(by='time')\n",
    "\n",
    "# Display the first few rows of the sorted dataframe\n",
    "print(df_steps_sorted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac0d4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the blood pressure dataset\n",
    "file_path_bp = \"blood_pressure_readings_ID24_cleaned.csv\"  # Replace with your actual file path\n",
    "df_bp = pd.read_csv(file_path_bp)\n",
    "\n",
    "# Select relevant columns\n",
    "df_bp = df_bp[['datetime_local', 'systolic', 'diastolic']]\n",
    "\n",
    "# Convert the datetime column to datetime format\n",
    "df_bp['datetime_local'] = pd.to_datetime(df_bp['datetime_local'])\n",
    "\n",
    "# Sort the dataframe by datetime in increasing order\n",
    "df_bp_sorted = df_bp.sort_values(by='datetime_local')\n",
    "\n",
    "# Add a binary classification column for BP spikes\n",
    "df_bp_sorted['BP_spike'] = ((df_bp_sorted['systolic'] > 130) | (df_bp_sorted['diastolic'] > 80)).astype(int)\n",
    "\n",
    "# Count the number of BP spikes and total records\n",
    "total_records = len(df_bp_sorted)\n",
    "bp_spike_count = df_bp_sorted['BP_spike'].sum()\n",
    "\n",
    "# Print summary\n",
    "print(f\"Total records: {total_records}\")\n",
    "print(f\"Number of BP spikes: {bp_spike_count}\")\n",
    "\n",
    "# Display the first few rows of the processed dataframe\n",
    "print(df_bp_sorted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554387f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the stress data dataset\n",
    "file_path_stress = \"questionnaire_responses_ID24.csv\"  # Replace with your actual file path\n",
    "df_stress = pd.read_csv(file_path_stress)\n",
    "\n",
    "# Select relevant columns\n",
    "df_stress = df_stress[['local_created_at', 'stressLevel_value']]\n",
    "\n",
    "# Convert the time column to datetime format\n",
    "df_stress['local_created_at'] = pd.to_datetime(df_stress['local_created_at'])\n",
    "\n",
    "# Sort the dataframe by time in increasing order\n",
    "df_stress_sorted = df_stress.sort_values(by='local_created_at')\n",
    "\n",
    "# Display the first few rows of the processed dataframe\n",
    "print(df_stress_sorted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbcb8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ðŸ“Œ **Step 1: Load Datasets**\n",
    "file_path_hr = \"hp24_hr.csv\"\n",
    "file_path_steps = \"hp24_steps.csv\"\n",
    "file_path_bp = \"blood_pressure_readings_ID24_cleaned.csv\"\n",
    "file_path_stress = \"questionnaire_responses_ID24.csv\"\n",
    "\n",
    "df_hr = pd.read_csv(file_path_hr)\n",
    "df_steps = pd.read_csv(file_path_steps)\n",
    "df_bp = pd.read_csv(file_path_bp)\n",
    "df_stress = pd.read_csv(file_path_stress)\n",
    "\n",
    "# ðŸ“Œ **Step 2: Convert timestamps to uniform `datetime64[ns]`**\n",
    "df_hr['time'] = pd.to_datetime(df_hr['time']).dt.tz_localize(None)\n",
    "df_steps['time'] = pd.to_datetime(df_steps['time']).dt.tz_localize(None)\n",
    "df_bp['datetime_local'] = pd.to_datetime(df_bp['datetime_local']).dt.tz_localize(None)\n",
    "df_stress['local_created_at'] = pd.to_datetime(df_stress['local_created_at']).dt.tz_localize(None)\n",
    "\n",
    "# ðŸ“Œ **Step 3: Remove Data from `2024-10-23` for BP & Stress**\n",
    "df_bp = df_bp[df_bp['datetime_local'].dt.date > pd.to_datetime(\"2024-09-03\").date()]\n",
    "df_stress = df_stress[df_stress['local_created_at'].dt.date > pd.to_datetime(\"2024-09-03\").date()]\n",
    "\n",
    "# ðŸ“Œ **Step 4: Sort datasets by time**\n",
    "df_hr_sorted = df_hr.sort_values(by='time')\n",
    "df_steps_sorted = df_steps.sort_values(by='time')\n",
    "df_bp_sorted = df_bp.sort_values(by='datetime_local')\n",
    "df_stress_sorted = df_stress.sort_values(by='local_created_at')\n",
    "\n",
    "# ðŸ“Œ **Step 5: Add BP spike classification and Tertile Classification**\n",
    "# Retain the threshold-based definition for reference:\n",
    "df_bp_sorted['BP_spike_threshold'] = ((df_bp_sorted['systolic'] > 130) | (df_bp_sorted['diastolic'] > 80)).astype(int)\n",
    "\n",
    "# New binary definition based on mean values:\n",
    "mean_systolic = df_bp_sorted['systolic'].mean()\n",
    "mean_diastolic = df_bp_sorted['diastolic'].mean()\n",
    "df_bp_sorted['BP_spike_mean'] = ((df_bp_sorted['systolic'] > mean_systolic) | (df_bp_sorted['diastolic'] > mean_diastolic)).astype(int)\n",
    "\n",
    "# Tertile classification for systolic and diastolic:\n",
    "systolic_lower = df_bp_sorted['systolic'].quantile(0.333)\n",
    "systolic_upper = df_bp_sorted['systolic'].quantile(0.667)\n",
    "diastolic_lower = df_bp_sorted['diastolic'].quantile(0.333)\n",
    "diastolic_upper = df_bp_sorted['diastolic'].quantile(0.667)\n",
    "\n",
    "def classify_bp(value, lower, upper):\n",
    "    if value < lower:\n",
    "        return \"low\"\n",
    "    elif value < upper:\n",
    "        return \"mid\"\n",
    "    else:\n",
    "        return \"high\"\n",
    "\n",
    "df_bp_sorted['systolic_tertile'] = df_bp_sorted['systolic'].apply(lambda x: classify_bp(x, systolic_lower, systolic_upper))\n",
    "df_bp_sorted['diastolic_tertile'] = df_bp_sorted['diastolic'].apply(lambda x: classify_bp(x, diastolic_lower, diastolic_upper))\n",
    "\n",
    "# ðŸ“Œ **Step 6: Merge HR & Steps Data Based on Nearest Timestamp**\n",
    "df_biosignals = pd.merge_asof(df_hr_sorted, df_steps_sorted, on='time', direction='backward', suffixes=('_hr', '_steps'))\n",
    "\n",
    "# ðŸ“Œ **Step 7: Compute Rolling Window Statistics for HR & Steps**\n",
    "df_biosignals.set_index('time', inplace=True)\n",
    "time_windows = [5, 10, 30, 60]  # Define time windows (minutes)\n",
    "for window in time_windows:\n",
    "    window_str = f\"{window}min\"\n",
    "    # Rolling windows here include the current row; if you need strictly previous data, consider shifting afterward.\n",
    "    df_biosignals[f'hr_mean_{window_str}'] = df_biosignals['value_hr'].rolling(f\"{window}min\").mean()\n",
    "    df_biosignals[f'hr_min_{window_str}'] = df_biosignals['value_hr'].rolling(f\"{window}min\").min()\n",
    "    df_biosignals[f'hr_max_{window_str}'] = df_biosignals['value_hr'].rolling(f\"{window}min\").max()\n",
    "    df_biosignals[f'hr_std_{window_str}'] = df_biosignals['value_hr'].rolling(f\"{window}min\").std()\n",
    "    df_biosignals[f'steps_total_{window_str}'] = df_biosignals['value_steps'].rolling(f\"{window}min\").sum()\n",
    "    df_biosignals[f'steps_mean_{window_str}'] = df_biosignals['value_steps'].rolling(f\"{window}min\").mean()\n",
    "    df_biosignals[f'steps_min_{window_str}'] = df_biosignals['value_steps'].rolling(f\"{window}min\").min()\n",
    "    df_biosignals[f'steps_max_{window_str}'] = df_biosignals['value_steps'].rolling(f\"{window}min\").max()\n",
    "    df_biosignals[f'steps_std_{window_str}'] = df_biosignals['value_steps'].rolling(f\"{window}min\").std()\n",
    "    df_biosignals[f'steps_diff_{window_str}'] = df_biosignals[f'steps_max_{window_str}'] - df_biosignals[f'steps_min_{window_str}']\n",
    "# Reset index after rolling computation\n",
    "df_biosignals.reset_index(inplace=True)\n",
    "\n",
    "# ðŸ“Œ **Step 8: Merge BP Data with HR & Steps Features**\n",
    "df_merged = pd.merge_asof(df_bp_sorted, df_biosignals, left_on='datetime_local', right_on='time', direction='backward')\n",
    "\n",
    "# ðŸ“Œ **Step 9: Incorporate Stress Data (Â±15 minutes window)**\n",
    "def extract_stress_features(bp_time, df_stress):\n",
    "    start_time = bp_time - pd.Timedelta(minutes=15)\n",
    "    end_time = bp_time + pd.Timedelta(minutes=15)\n",
    "    stress_values = df_stress[(df_stress['local_created_at'] >= start_time) &\n",
    "                              (df_stress['local_created_at'] <= end_time)]['stressLevel_value']\n",
    "    return pd.Series({\n",
    "        'stress_mean': stress_values.mean(),\n",
    "        'stress_min': stress_values.min(),\n",
    "        'stress_max': stress_values.max(),\n",
    "        'stress_std': stress_values.std()\n",
    "    })\n",
    "\n",
    "df_stress_features = df_bp_sorted['datetime_local'].apply(lambda x: extract_stress_features(x, df_stress_sorted))\n",
    "df_merged = pd.concat([df_merged, df_stress_features], axis=1)\n",
    "\n",
    "# ðŸ“Œ **Step 10: Create Additional Engineered Features**\n",
    "\n",
    "# âœ… Lagged Features: only using previous data (based on BP_spike_mean and other past features)\n",
    "lag_features = ['stress_mean', 'BP_spike_mean', 'hr_mean_5min', 'steps_total_10min']\n",
    "for feature in lag_features:\n",
    "    for lag in [1, 3, 5]:\n",
    "        df_merged[f'{feature}_lag_{lag}'] = df_merged[feature].shift(lag)\n",
    "\n",
    "# âœ… Feature Interactions\n",
    "df_merged['hr_steps_ratio'] = df_merged['hr_mean_5min'] / (df_merged['steps_total_10min'] + 1)\n",
    "df_merged['stress_weighted_hr'] = df_merged['hr_mean_5min'] * df_merged['stress_mean']\n",
    "df_merged['stress_steps_ratio'] = df_merged['stress_mean'] / (df_merged['steps_total_10min'] + 1)\n",
    "df_merged['steps_hr_variability_ratio'] = df_merged['steps_std_10min'] / (df_merged['hr_std_10min'] + 1e-5)\n",
    "\n",
    "# âœ… Rolling Aggregations\n",
    "df_merged['hr_mean_rolling_3'] = df_merged['hr_mean_5min'].rolling(3).mean()\n",
    "df_merged['steps_total_rolling_5'] = df_merged['steps_total_10min'].rolling(5).mean()\n",
    "df_merged['hr_std_rolling_3'] = df_merged['hr_std_10min'].rolling(3).std()\n",
    "df_merged['cumulative_stress_30min'] = df_merged['stress_mean'].rolling(3).sum()\n",
    "df_merged['cumulative_steps_30min'] = df_merged['steps_total_10min'].rolling(3).sum()\n",
    "\n",
    "# âœ… Contextual Features\n",
    "df_merged['hour_of_day'] = df_merged['datetime_local'].dt.hour\n",
    "df_merged['day_of_week'] = df_merged['datetime_local'].dt.dayofweek\n",
    "df_merged['is_working_hours'] = df_merged['hour_of_day'].between(9, 17).astype(int)\n",
    "df_merged['is_weekend'] = (df_merged['day_of_week'] >= 5).astype(int)\n",
    "\n",
    "# âœ… Time Since Last BP Spike (based on BP_spike_mean)\n",
    "# Use previous row's timestamp (if it was a spike) by shifting the datetime column.\n",
    "df_merged['last_spike_time'] = df_merged['datetime_local'].shift(1).where(df_merged['BP_spike_mean'].shift(1) == 1)\n",
    "df_merged['last_spike_time'] = df_merged['last_spike_time'].ffill()\n",
    "df_merged['time_since_last_BP_spike'] = (df_merged['datetime_local'] - df_merged['last_spike_time']).dt.total_seconds() / 60\n",
    "df_merged.drop(columns=['last_spike_time'], inplace=True)\n",
    "\n",
    "# âœ… Drop Direct Current Row Measurements to Prevent Data Leakage\n",
    "# We drop the raw systolic and diastolic values and the threshold-based spike,\n",
    "# but now keep the tertile classifications for use in the 3-class problem.\n",
    "drop_cols = ['systolic', 'diastolic', 'BP_spike_threshold']\n",
    "df_merged.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "# ðŸ“Œ **Step 11: Handle Missing Values**\n",
    "df_merged.ffill(inplace=True)\n",
    "df_merged.bfill(inplace=True)  # Fixes rolling feature NaNs at the beginning\n",
    "\n",
    "# ðŸ“Œ **Step 12: Save Processed Dataset**\n",
    "df_merged.to_csv(\"processed_bp_prediction_data_multiclass.csv\", index=False)\n",
    "print(\"âœ… Final dataset saved as 'processed_bp_prediction_data_multiclass.csv'.\")\n",
    "print(df_merged.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4853cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 01m 51s]\n",
      "val_systolic_systolic_auc: 0.6725838780403137\n",
      "\n",
      "Best val_systolic_systolic_auc So Far: 0.682716429233551\n",
      "Total elapsed time: 00h 32m 05s\n",
      "WARNING:tensorflow:From C:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\saving\\legacy\\save.py:538: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.\n",
      "\n",
      "ðŸ”¹ Best Multi-Output Model Hyperparameters:\n",
      "{'lstm_units': 256, 'dropout_rate': 0.2, 'lstm_units_2': 48, 'attention_variant': 'selfattention', 'shared_dense_units': 32, 'systolic_dense_units': 16, 'diastolic_dense_units': 16, 'learning_rate': 0.001, 'num_heads': 4, 'key_dim': 64}\n",
      "Test Loss & AUROC (Systolic, Diastolic): [2.113337278366089, 1.0713911056518555, 1.041946291923523, 0.682716429233551, 0.6651015877723694]\n",
      "Total training time: 1944.48 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, BatchNormalization, Bidirectional, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import ADASYN\n",
    "import kerastuner as kt\n",
    "\n",
    "#############################################\n",
    "# Custom Attention Layers\n",
    "#############################################\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\",\n",
    "                                 shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\",\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\",\n",
    "                                 shape=(input_shape[1], 1),\n",
    "                                 initializer=\"zeros\",\n",
    "                                 trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "    def call(self, x):\n",
    "        e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)\n",
    "        a = tf.keras.backend.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "        return tf.keras.backend.sum(output, axis=1)\n",
    "\n",
    "class MultiHeadAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, key_dim, **kwargs):\n",
    "        super(MultiHeadAttentionLayer, self).__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.key_dim)\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.mha(query=inputs, key=inputs, value=inputs)\n",
    "        return tf.reduce_mean(attn_output, axis=1)\n",
    "\n",
    "class SelfAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SelfAttentionLayer, self).__init__(**kwargs)\n",
    "        self.att = tf.keras.layers.Attention()\n",
    "    def call(self, x):\n",
    "        att_out = self.att([x, x])\n",
    "        return tf.reduce_mean(att_out, axis=1)\n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, key_dim, ff_dim, rate=0.1, **kwargs):\n",
    "        super(TransformerBlock, self).__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.ff_dim = ff_dim\n",
    "        self.rate = rate\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "    def build(self, input_shape):\n",
    "        self.embed_dim = input_shape[-1]\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(self.ff_dim, activation='relu'),\n",
    "            Dense(self.embed_dim)\n",
    "        ])\n",
    "        super(TransformerBlock, self).build(input_shape)\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.mha(query=inputs, key=inputs, value=inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "#############################################\n",
    "# 1. Load Processed Dataset and Prepare Targets\n",
    "#############################################\n",
    "# Processed CSV must include the tertile columns computed earlier: \"systolic_tertile\" and \"diastolic_tertile\".\n",
    "df = pd.read_csv(\"processed_bp_prediction_data_multiclass.csv\")\n",
    "\n",
    "# Define features (ensure target columns are excluded)\n",
    "features = [\n",
    "    'hr_mean_5min', 'hr_min_5min', 'hr_max_5min', 'hr_std_5min',\n",
    "    'steps_total_5min', 'steps_mean_5min', 'steps_min_5min', 'steps_max_5min', 'steps_std_5min', 'steps_diff_5min',\n",
    "    'hr_mean_10min', 'hr_min_10min', 'hr_max_10min', 'hr_std_10min',\n",
    "    'steps_total_10min', 'steps_mean_10min', 'steps_min_10min', 'steps_max_10min', 'steps_std_10min', 'steps_diff_10min',\n",
    "    'hr_mean_30min', 'hr_min_30min', 'hr_max_30min', 'hr_std_30min',\n",
    "    'steps_total_30min', 'steps_mean_30min', 'steps_min_30min', 'steps_max_30min', 'steps_std_30min', 'steps_diff_30min',\n",
    "    'hr_mean_60min', 'hr_min_60min', 'hr_max_60min', 'hr_std_60min',\n",
    "    'steps_total_60min', 'steps_mean_60min', 'steps_min_60min', 'steps_max_60min', 'steps_std_60min', 'steps_diff_60min',\n",
    "    'stress_mean', 'stress_min', 'stress_max', 'stress_std',\n",
    "    'stress_mean_lag_1', 'stress_mean_lag_3', 'stress_mean_lag_5',\n",
    "    'BP_spike_mean_lag_1', 'BP_spike_mean_lag_3', 'BP_spike_mean_lag_5',\n",
    "    'hr_mean_5min_lag_1', 'hr_mean_5min_lag_3', 'hr_mean_5min_lag_5',\n",
    "    'steps_total_10min_lag_1', 'steps_total_10min_lag_3', 'steps_total_10min_lag_5',\n",
    "    'hr_steps_ratio', 'stress_weighted_hr', 'stress_steps_ratio', 'steps_hr_variability_ratio',\n",
    "    'hr_mean_rolling_3', 'steps_total_rolling_5', 'hr_std_rolling_3',\n",
    "    'cumulative_stress_30min', 'cumulative_steps_30min',\n",
    "    'hour_of_day', 'day_of_week', 'is_working_hours', 'is_weekend',\n",
    "    'time_since_last_BP_spike'\n",
    "]\n",
    "\n",
    "# Our targets: the tertile classifications for systolic and diastolic values\n",
    "target_systolic = \"systolic_tertile\"\n",
    "target_diastolic = \"diastolic_tertile\"\n",
    "\n",
    "# Retain only needed columns\n",
    "df = df[[\"datetime_local\"] + features + [target_systolic, target_diastolic]]\n",
    "df[features] = df[features].apply(pd.to_numeric, errors='coerce')\n",
    "df['datetime_local'] = pd.to_datetime(df['datetime_local'])\n",
    "\n",
    "# Split train and test by time\n",
    "train_cutoff = df['datetime_local'].min() + pd.Timedelta(days=20)\n",
    "train_data = df[df['datetime_local'] < train_cutoff]\n",
    "test_data = df[df['datetime_local'] >= train_cutoff]\n",
    "\n",
    "X_train = train_data[features].copy()\n",
    "X_test = test_data[features].copy()\n",
    "\n",
    "# One-hot encode targets for multi-class classification\n",
    "y_train_systolic = pd.get_dummies(train_data[target_systolic])\n",
    "y_train_diastolic = pd.get_dummies(train_data[target_diastolic])\n",
    "y_test_systolic = pd.get_dummies(test_data[target_systolic])\n",
    "y_test_diastolic = pd.get_dummies(test_data[target_diastolic])\n",
    "\n",
    "# Ensure consistent ordering of columns\n",
    "order = ['low', 'mid', 'high']\n",
    "y_train_systolic = y_train_systolic[order]\n",
    "y_train_diastolic = y_train_diastolic[order]\n",
    "y_test_systolic = y_test_systolic[order]\n",
    "y_test_diastolic = y_test_diastolic[order]\n",
    "\n",
    "# Create dictionary targets for the multi-output model\n",
    "y_train_multi = {'systolic': y_train_systolic.values, 'diastolic': y_train_diastolic.values}\n",
    "y_test_multi = {'systolic': y_test_systolic.values, 'diastolic': y_test_diastolic.values}\n",
    "\n",
    "#############################################\n",
    "# 2. Scaling and Reshaping for LSTM\n",
    "#############################################\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
    "\n",
    "#############################################\n",
    "# 3. Build Multi-Output (Multi-Task) LSTM Model with Keras Tuner\n",
    "#############################################\n",
    "def build_model(hp):\n",
    "    inputs = tf.keras.Input(shape=(X_train_lstm.shape[1], 1))\n",
    "    \n",
    "    # Shared LSTM layers\n",
    "    lstm_units = hp.Int('lstm_units', min_value=64, max_value=256, step=32)\n",
    "    x = Bidirectional(LSTM(lstm_units, return_sequences=True))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    dropout_rate = hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    lstm_units_2 = hp.Int('lstm_units_2', min_value=32, max_value=128, step=16)\n",
    "    x = LSTM(lstm_units_2, return_sequences=True)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Attention variant selection\n",
    "    att_variant = hp.Choice('attention_variant', ['custom', 'multihead', 'selfattention', 'transformer'])\n",
    "    if att_variant == 'custom':\n",
    "        att_out = AttentionLayer()(x)\n",
    "    elif att_variant == 'multihead':\n",
    "        num_heads = hp.Int('num_heads', min_value=1, max_value=4, step=1)\n",
    "        key_dim = hp.Int('key_dim', min_value=16, max_value=64, step=16)\n",
    "        att_out = MultiHeadAttentionLayer(num_heads=num_heads, key_dim=key_dim)(x)\n",
    "    elif att_variant == 'selfattention':\n",
    "        att_out = SelfAttentionLayer()(x)\n",
    "    else:  # transformer variant\n",
    "        trans_num_heads = hp.Int('trans_num_heads', min_value=1, max_value=4, step=1)\n",
    "        trans_key_dim = hp.Int('trans_key_dim', min_value=16, max_value=64, step=16)\n",
    "        ff_dim = hp.Int('ff_dim', min_value=32, max_value=128, step=32)\n",
    "        transformer_block = TransformerBlock(num_heads=trans_num_heads, key_dim=trans_key_dim, ff_dim=ff_dim, rate=dropout_rate)\n",
    "        att_out = transformer_block(x)\n",
    "        att_out = GlobalAveragePooling1D()(att_out)\n",
    "    \n",
    "    # Optional shared dense layer\n",
    "    shared_dense_units = hp.Int('shared_dense_units', min_value=32, max_value=128, step=32)\n",
    "    shared = Dense(shared_dense_units, activation='relu')(att_out)\n",
    "    shared = Dropout(dropout_rate)(shared)\n",
    "    \n",
    "    # Systolic branch\n",
    "    systolic_dense_units = hp.Int('systolic_dense_units', min_value=16, max_value=64, step=16)\n",
    "    systolic_branch = Dense(systolic_dense_units, activation='relu')(shared)\n",
    "    systolic_branch = Dropout(dropout_rate)(systolic_branch)\n",
    "    systolic_output = Dense(3, activation='softmax', name='systolic')(systolic_branch)\n",
    "    \n",
    "    # Diastolic branch\n",
    "    diastolic_dense_units = hp.Int('diastolic_dense_units', min_value=16, max_value=64, step=16)\n",
    "    diastolic_branch = Dense(diastolic_dense_units, activation='relu')(shared)\n",
    "    diastolic_branch = Dropout(dropout_rate)(diastolic_branch)\n",
    "    diastolic_output = Dense(3, activation='softmax', name='diastolic')(diastolic_branch)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=[systolic_output, diastolic_output])\n",
    "    \n",
    "    # Create one-vs-rest AUROC metrics for each output.\n",
    "    systolic_auc = tf.keras.metrics.AUC(multi_label=True, num_labels=3, curve='ROC', name='systolic_auc')\n",
    "    diastolic_auc = tf.keras.metrics.AUC(multi_label=True, num_labels=3, curve='ROC', name='diastolic_auc')\n",
    "    \n",
    "    learning_rate = hp.Choice('learning_rate', [0.001, 0.0005, 0.0001])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss={'systolic': 'categorical_crossentropy', 'diastolic': 'categorical_crossentropy'},\n",
    "                  metrics={'systolic': systolic_auc, 'diastolic': diastolic_auc})\n",
    "    return model\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective=kt.Objective('val_systolic_systolic_auc', direction='max'),\n",
    "    max_trials=20,\n",
    "    executions_per_trial=1,\n",
    "    directory='lstm_tuner_multioutput',\n",
    "    project_name='bp_tertile_prediction',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "tuner.search(X_train_lstm, y_train_multi,\n",
    "             epochs=50,\n",
    "             batch_size=32,\n",
    "             validation_data=(X_test_lstm, y_test_multi))\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "print(\"ðŸ”¹ Best Multi-Output Model Hyperparameters:\")\n",
    "print(tuner.get_best_hyperparameters(num_trials=1)[0].values)\n",
    "\n",
    "#############################################\n",
    "# 4. Evaluate the Best Model on the Test Set\n",
    "#############################################\n",
    "results = best_model.evaluate(X_test_lstm, y_test_multi, verbose=0)\n",
    "print(f\"Test Loss & AUROC (Systolic, Diastolic): {results}\")\n",
    "\n",
    "#############################################\n",
    "# 5. Print Total Training Time\n",
    "#############################################\n",
    "end_time = time.time()\n",
    "print(\"Total training time: {:.2f} seconds\".format(end_time - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0183d5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”¹ BP Spike Counts Before Resampling:\")\n",
    "print(f\"   - Training Set: {sum(y_train)} spikes out of {len(y_train)} samples ({sum(y_train)/len(y_train)*100:.2f}%)\")\n",
    "print(f\"   - Test Set: {sum(y_test)} spikes out of {len(y_test)} samples ({sum(y_test)/len(y_test)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4d2776f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 02m 03s]\n",
      "val_systolic_systolic_auc: 0.6322672963142395\n",
      "\n",
      "Best val_systolic_systolic_auc So Far: 0.6723673343658447\n",
      "Total elapsed time: 00h 28m 05s\n",
      "\n",
      "Trial 19 complete: trial.history not available. Best objective (systolic) is 0.6323\n",
      "ðŸ”¹ Best Multi-Output Model Hyperparameters:\n",
      "{'lstm_units': 160, 'dropout_rate': 0.2, 'lstm_units_2': 32, 'attention_variant': 'selfattention', 'shared_dense_units': 96, 'systolic_dense_units': 32, 'diastolic_dense_units': 64, 'learning_rate': 0.0001, 'num_heads': 3, 'key_dim': 16}\n",
      "Test Evaluation Metrics:\n",
      "Overall Loss: 2.1921\n",
      "Systolic Loss: 1.0933\n",
      "Diastolic Loss: 1.0988\n",
      "Systolic AUROC: 0.6724\n",
      "Diastolic AUROC: 0.6278\n",
      "Total training time: 1696.08 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, BatchNormalization, Bidirectional, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import ADASYN\n",
    "import kerastuner as kt\n",
    "\n",
    "#############################################\n",
    "# Custom Attention Layers\n",
    "#############################################\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\",\n",
    "                                 shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\",\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\",\n",
    "                                 shape=(input_shape[1], 1),\n",
    "                                 initializer=\"zeros\",\n",
    "                                 trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "    def call(self, x):\n",
    "        e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)\n",
    "        a = tf.keras.backend.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "        return tf.keras.backend.sum(output, axis=1)\n",
    "\n",
    "class MultiHeadAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, key_dim, **kwargs):\n",
    "        super(MultiHeadAttentionLayer, self).__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.key_dim)\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.mha(query=inputs, key=inputs, value=inputs)\n",
    "        return tf.reduce_mean(attn_output, axis=1)\n",
    "\n",
    "class SelfAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SelfAttentionLayer, self).__init__(**kwargs)\n",
    "        self.att = tf.keras.layers.Attention()\n",
    "    def call(self, x):\n",
    "        att_out = self.att([x, x])\n",
    "        return tf.reduce_mean(att_out, axis=1)\n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, key_dim, ff_dim, rate=0.1, **kwargs):\n",
    "        super(TransformerBlock, self).__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.ff_dim = ff_dim\n",
    "        self.rate = rate\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "    def build(self, input_shape):\n",
    "        self.embed_dim = input_shape[-1]\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(self.ff_dim, activation='relu'),\n",
    "            Dense(self.embed_dim)\n",
    "        ])\n",
    "        super(TransformerBlock, self).build(input_shape)\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.mha(query=inputs, key=inputs, value=inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "#############################################\n",
    "# Custom Tuner to Print Both AUROC Metrics at Trial End\n",
    "#############################################\n",
    "class MyRandomSearch(kt.RandomSearch):\n",
    "    def on_trial_end(self, trial):\n",
    "        # Call parent's on_trial_end first.\n",
    "        super().on_trial_end(trial)\n",
    "        # Try to access trial history if available.\n",
    "        if hasattr(trial, 'history') and trial.history is not None:\n",
    "            history = trial.history.history\n",
    "            best_val_systolic = max(history.get(\"val_systolic_systolic_auc\", [float('nan')]))\n",
    "            best_val_diastolic = max(history.get(\"val_diastolic_diastolic_auc\", [float('nan')]))\n",
    "            print(f\"\\nTrial {trial.trial_id} complete:\")\n",
    "            print(f\"   Best val_systolic_systolic_auc: {best_val_systolic:.4f}\")\n",
    "            print(f\"   Best val_diastolic_diastolic_auc: {best_val_diastolic:.4f}\")\n",
    "        else:\n",
    "            # If history is not available, fallback: print best objective (systolic only)\n",
    "            print(f\"\\nTrial {trial.trial_id} complete: trial.history not available. Best objective (systolic) is {trial.score:.4f}\")\n",
    "\n",
    "#############################################\n",
    "# 1. Load Processed Dataset and Prepare Targets\n",
    "#############################################\n",
    "# The processed CSV must include the tertile columns \"systolic_tertile\" and \"diastolic_tertile\".\n",
    "df = pd.read_csv(\"processed_bp_prediction_data_multiclass.csv\")\n",
    "\n",
    "# Define features (ensure target columns are excluded)\n",
    "features = [\n",
    "    'hr_mean_5min', 'hr_min_5min', 'hr_max_5min', 'hr_std_5min',\n",
    "    'steps_total_5min', 'steps_mean_5min', 'steps_min_5min', 'steps_max_5min', 'steps_std_5min', 'steps_diff_5min',\n",
    "    'hr_mean_10min', 'hr_min_10min', 'hr_max_10min', 'hr_std_10min',\n",
    "    'steps_total_10min', 'steps_mean_10min', 'steps_min_10min', 'steps_max_10min', 'steps_std_10min', 'steps_diff_10min',\n",
    "    'hr_mean_30min', 'hr_min_30min', 'hr_max_30min', 'hr_std_30min',\n",
    "    'steps_total_30min', 'steps_mean_30min', 'steps_min_30min', 'steps_max_30min', 'steps_std_30min', 'steps_diff_30min',\n",
    "    'hr_mean_60min', 'hr_min_60min', 'hr_max_60min', 'hr_std_60min',\n",
    "    'steps_total_60min', 'steps_mean_60min', 'steps_min_60min', 'steps_max_60min', 'steps_std_60min', 'steps_diff_60min',\n",
    "    'stress_mean', 'stress_min', 'stress_max', 'stress_std',\n",
    "    'stress_mean_lag_1', 'stress_mean_lag_3', 'stress_mean_lag_5',\n",
    "    'BP_spike_mean_lag_1', 'BP_spike_mean_lag_3', 'BP_spike_mean_lag_5',\n",
    "    'hr_mean_5min_lag_1', 'hr_mean_5min_lag_3', 'hr_mean_5min_lag_5',\n",
    "    'steps_total_10min_lag_1', 'steps_total_10min_lag_3', 'steps_total_10min_lag_5',\n",
    "    'hr_steps_ratio', 'stress_weighted_hr', 'stress_steps_ratio', 'steps_hr_variability_ratio',\n",
    "    'hr_mean_rolling_3', 'steps_total_rolling_5', 'hr_std_rolling_3',\n",
    "    'cumulative_stress_30min', 'cumulative_steps_30min',\n",
    "    'hour_of_day', 'day_of_week', 'is_working_hours', 'is_weekend',\n",
    "    'time_since_last_BP_spike'\n",
    "]\n",
    "\n",
    "# Our targets: the tertile classifications for systolic and diastolic values\n",
    "target_systolic = \"systolic_tertile\"\n",
    "target_diastolic = \"diastolic_tertile\"\n",
    "\n",
    "# Retain only needed columns\n",
    "df = df[[\"datetime_local\"] + features + [target_systolic, target_diastolic]]\n",
    "df[features] = df[features].apply(pd.to_numeric, errors='coerce')\n",
    "df['datetime_local'] = pd.to_datetime(df['datetime_local'])\n",
    "\n",
    "# Split train and test by time\n",
    "train_cutoff = df['datetime_local'].min() + pd.Timedelta(days=20)\n",
    "train_data = df[df['datetime_local'] < train_cutoff]\n",
    "test_data = df[df['datetime_local'] >= train_cutoff]\n",
    "\n",
    "X_train = train_data[features].copy()\n",
    "X_test = test_data[features].copy()\n",
    "\n",
    "# One-hot encode targets for multi-class classification\n",
    "y_train_systolic = pd.get_dummies(train_data[target_systolic])\n",
    "y_train_diastolic = pd.get_dummies(train_data[target_diastolic])\n",
    "y_test_systolic = pd.get_dummies(test_data[target_systolic])\n",
    "y_test_diastolic = pd.get_dummies(test_data[target_diastolic])\n",
    "\n",
    "# Ensure consistent ordering of columns\n",
    "order = ['low', 'mid', 'high']\n",
    "y_train_systolic = y_train_systolic[order]\n",
    "y_train_diastolic = y_train_diastolic[order]\n",
    "y_test_systolic = y_test_systolic[order]\n",
    "y_test_diastolic = y_test_diastolic[order]\n",
    "\n",
    "# Create dictionary targets for the multi-output model\n",
    "y_train_multi = {'systolic': y_train_systolic.values, 'diastolic': y_train_diastolic.values}\n",
    "y_test_multi = {'systolic': y_test_systolic.values, 'diastolic': y_test_diastolic.values}\n",
    "\n",
    "#############################################\n",
    "# 2. Scaling and Reshaping for LSTM\n",
    "#############################################\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
    "\n",
    "#############################################\n",
    "# 3. Build Multi-Output (Multi-Task) LSTM Model with Keras Tuner\n",
    "#############################################\n",
    "def build_model(hp):\n",
    "    inputs = tf.keras.Input(shape=(X_train_lstm.shape[1], 1))\n",
    "    \n",
    "    # Shared LSTM layers\n",
    "    lstm_units = hp.Int('lstm_units', min_value=64, max_value=256, step=32)\n",
    "    x = Bidirectional(LSTM(lstm_units, return_sequences=True))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    dropout_rate = hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    lstm_units_2 = hp.Int('lstm_units_2', min_value=32, max_value=128, step=16)\n",
    "    x = LSTM(lstm_units_2, return_sequences=True)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Attention variant selection\n",
    "    att_variant = hp.Choice('attention_variant', ['custom', 'multihead', 'selfattention', 'transformer'])\n",
    "    if att_variant == 'custom':\n",
    "        att_out = AttentionLayer()(x)\n",
    "    elif att_variant == 'multihead':\n",
    "        num_heads = hp.Int('num_heads', min_value=1, max_value=4, step=1)\n",
    "        key_dim = hp.Int('key_dim', min_value=16, max_value=64, step=16)\n",
    "        att_out = MultiHeadAttentionLayer(num_heads=num_heads, key_dim=key_dim)(x)\n",
    "    elif att_variant == 'selfattention':\n",
    "        att_out = SelfAttentionLayer()(x)\n",
    "    else:  # transformer variant\n",
    "        trans_num_heads = hp.Int('trans_num_heads', min_value=1, max_value=4, step=1)\n",
    "        trans_key_dim = hp.Int('trans_key_dim', min_value=16, max_value=64, step=16)\n",
    "        ff_dim = hp.Int('ff_dim', min_value=32, max_value=128, step=32)\n",
    "        transformer_block = TransformerBlock(num_heads=trans_num_heads, key_dim=trans_key_dim, ff_dim=ff_dim, rate=dropout_rate)\n",
    "        att_out = transformer_block(x)\n",
    "        att_out = GlobalAveragePooling1D()(att_out)\n",
    "    \n",
    "    # Optional shared dense layer\n",
    "    shared_dense_units = hp.Int('shared_dense_units', min_value=32, max_value=128, step=32)\n",
    "    shared = Dense(shared_dense_units, activation='relu')(att_out)\n",
    "    shared = Dropout(dropout_rate)(shared)\n",
    "    \n",
    "    # Systolic branch\n",
    "    systolic_dense_units = hp.Int('systolic_dense_units', min_value=16, max_value=64, step=16)\n",
    "    systolic_branch = Dense(systolic_dense_units, activation='relu')(shared)\n",
    "    systolic_branch = Dropout(dropout_rate)(systolic_branch)\n",
    "    systolic_output = Dense(3, activation='softmax', name='systolic')(systolic_branch)\n",
    "    \n",
    "    # Diastolic branch\n",
    "    diastolic_dense_units = hp.Int('diastolic_dense_units', min_value=16, max_value=64, step=16)\n",
    "    diastolic_branch = Dense(diastolic_dense_units, activation='relu')(shared)\n",
    "    diastolic_branch = Dropout(dropout_rate)(diastolic_branch)\n",
    "    diastolic_output = Dense(3, activation='softmax', name='diastolic')(diastolic_branch)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=[systolic_output, diastolic_output])\n",
    "    \n",
    "    # Create one-vs-rest AUROC metrics for each output.\n",
    "    systolic_auc = tf.keras.metrics.AUC(multi_label=True, num_labels=3, curve='ROC', name='systolic_auc')\n",
    "    diastolic_auc = tf.keras.metrics.AUC(multi_label=True, num_labels=3, curve='ROC', name='diastolic_auc')\n",
    "    \n",
    "    learning_rate = hp.Choice('learning_rate', [0.001, 0.0005, 0.0001])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss={'systolic': 'categorical_crossentropy', 'diastolic': 'categorical_crossentropy'},\n",
    "                  metrics={'systolic': systolic_auc, 'diastolic': diastolic_auc})\n",
    "    return model\n",
    "\n",
    "tuner = MyRandomSearch(\n",
    "    build_model,\n",
    "    objective=kt.Objective('val_systolic_systolic_auc', direction='max'),\n",
    "    max_trials=20,\n",
    "    executions_per_trial=1,\n",
    "    directory='lstm_tuner_multioutput',\n",
    "    project_name='bp_tertile_prediction',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "tuner.search(X_train_lstm, y_train_multi,\n",
    "             epochs=50,\n",
    "             batch_size=32,\n",
    "             validation_data=(X_test_lstm, y_test_multi))\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "print(\"ðŸ”¹ Best Multi-Output Model Hyperparameters:\")\n",
    "print(tuner.get_best_hyperparameters(num_trials=1)[0].values)\n",
    "\n",
    "#############################################\n",
    "# 4. Evaluate the Best Model on the Test Set\n",
    "#############################################\n",
    "results = best_model.evaluate(X_test_lstm, y_test_multi, verbose=0)\n",
    "metrics = best_model.metrics_names\n",
    "result_dict = dict(zip(metrics, results))\n",
    "\n",
    "print(\"Test Evaluation Metrics:\")\n",
    "print(f\"Overall Loss: {result_dict['loss']:.4f}\")\n",
    "print(f\"Systolic Loss: {result_dict['systolic_loss']:.4f}\")\n",
    "print(f\"Diastolic Loss: {result_dict['diastolic_loss']:.4f}\")\n",
    "print(f\"Systolic AUROC: {result_dict['systolic_systolic_auc']:.4f}\")\n",
    "print(f\"Diastolic AUROC: {result_dict['diastolic_diastolic_auc']:.4f}\")\n",
    "\n",
    "#############################################\n",
    "# 5. Print Total Training Time\n",
    "#############################################\n",
    "end_time = time.time()\n",
    "print(\"Total training time: {:.2f} seconds\".format(end_time - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9911a0e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
